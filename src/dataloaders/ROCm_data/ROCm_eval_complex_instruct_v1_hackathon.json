[{"file": "test_gemm_fusion.py", "target_kernel_name": "gemm_fusion_kernel", "instruction": "\nYou are an expert in triton programming language. You will be given the function definition for the `gemm_fusion_kernel`. Your task is to complete the kernel code. Only complete the kernel code in the function definition, DONT remove any python imports or helper utils in the instruction/code provided, DONT change/interfere with the provided function definition and parameter list.\n\nThis kernel, `gemm_fusion_kernel`,  is designed to perform a fused matrix multiplication operation.\n\n**Your objective is to implement the body of `gemm_fusion_kernel`.**\n\nYou must ensure that:\n1.  All arguments received by `gemm_fusion_kernel` are kept intact and not modified.\n2. Provide you final code in ```python code block. \nExample:\n```python\n<YOUR-CODE-HERE>\n```\nThe full definition for `gemm_fusion_kernel` and relevant helper utilities are provided in the context below. You only need to complete the code for `gemm_fusion_kernel` whilst keeping other things intact.\n\n\n######################################## Imports #######################################\nimport pytest\nimport torch\n\nimport triton\nimport triton.language as tl\n\n######################################## Imports #######################################\n\n\n\n@triton.jit\ndef gemm_fusion_kernel(A, B, C, E,  #\n                       M, N, K,  #\n                       stride_am, stride_ak, stride_bn, stride_bk, stride_cn, stride_ck, stride_em, stride_ek,  #\n                       BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr):\n    \"\"\"\n    This Triton kernel is designed to perform a fused matrix multiplication operation.\n    It computes E = (A @ B^T) @ C, where A, B, C, and E are matrices.\n    The computation is performed in a tiled manner to optimize for memory access patterns\n    and leverage the parallelism of GPU architectures.\n\n    Each program instance (kernel launch) processes a tile of the output matrix E,\n    corresponding to a `BLOCK_M` strip of rows from matrix A. It iterates over\n    tiles of B and C along their N dimension to compute the intermediate product\n    (A_tile @ B_tile^T) and then accumulates the result with C_tile into E_tile.\n\n    Args:\n        A: Pointer to the input matrix A. Expected shape (M, K_common).\n        B: Pointer to the input matrix B. Expected shape (N, K_common).\n        C: Pointer to the input matrix C. Expected shape (N, K_out).\n        E: Pointer to the output matrix E, where the result E = (A @ B^T) @ C is stored. Expected shape (M, K_out).\n        M: The number of rows in matrix A and matrix E.\n        N: The number of rows in matrix B and matrix C. This is also the dimension\n           over which the product (A @ B^T) and C are contracted.\n        K: This parameter represents two potentially different dimensions depending on context,\n           but given the block shapes, it is used as K_common for A and B, and K_out for C and E.\n           Specifically:\n           - For A and B, it's the common dimension (K_common) for A @ B^T.\n           - For C and E, it's the output column dimension (K_out).\n           The kernel structure (BLOCK_K used for all) implies K_common == K_out.\n        stride_am: The stride (in number of elements) for matrix A along the M dimension (row stride).\n        stride_ak: The stride (in number of elements) for matrix A along the K dimension (column stride).\n        stride_bn: The stride (in number of elements) for matrix B along the N dimension (row stride).\n        stride_bk: The stride (in number of elements) for matrix B along the K dimension (column stride).\n        stride_cn: The stride (in number of elements) for matrix C along the N dimension (row stride).\n        stride_ck: The stride (in number of elements) for matrix C along the K dimension (column stride).\n        stride_em: The stride (in number of elements) for matrix E along the M dimension (row stride).\n        stride_ek: The stride (in number of elements) for matrix E along the K dimension (column stride).\n        BLOCK_M: tl.constexpr, the tile size for the M dimension. Each kernel instance\n                   processes a block of `BLOCK_M` rows from A and E.\n        BLOCK_N: tl.constexpr, the tile size for the N dimension. The kernel iterates\n                   over B and C in blocks of `BLOCK_N` along their N dimension.\n        BLOCK_K: tl.constexpr, the tile size for the K dimension. This is the block size\n                   for the common dimension in A@B^T and the output column dimension\n                   for C and E.\n    \"\"\"\n    # Your code here\n\n\n", "label": "# Usage Instruction: python3 -m pytest test_gemm_fusion.py\n\n# Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n#\n# Permission is hereby granted, free of charge, to any person obtaining\n# a copy of this software and associated documentation files\n# (the \"Software\"), to deal in the Software without restriction,\n# including without limitation the rights to use, copy, modify, merge,\n# publish, distribute, sublicense, and/or sell copies of the Software,\n# and to permit persons to whom the Software is furnished to do so,\n# subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be\n# included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n# MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n# IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n# CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n# SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\n######################################## Imports#######################################\nimport pytest\nimport torch\n\nimport triton\nimport triton.language as tl\n\n######################################## Imports#######################################\n\n\n\n@triton.jit\ndef gemm_fusion_kernel(A, B, C, E,  #\n                       M, N, K,  #\n                       stride_am, stride_ak, stride_bn, stride_bk, stride_cn, stride_ck, stride_em, stride_ek,  #\n                       BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr):\n    pid = tl.program_id(0)\n\n    a_tile_ptr = tl.make_block_ptr(base=A, shape=(M, K), strides=(stride_am, stride_ak), offsets=(pid * BLOCK_M, 0),\n                                   block_shape=(BLOCK_M, BLOCK_K), order=(1, 0))\n    b_tile_ptr = tl.make_block_ptr(base=B, shape=(N, K), strides=(stride_bn, stride_bk), offsets=(0, 0),\n                                   block_shape=(BLOCK_N, BLOCK_K), order=(1, 0))\n    c_tile_ptr = tl.make_block_ptr(base=C, shape=(N, K), strides=(stride_cn, stride_ck), offsets=(0, 0),\n                                   block_shape=(BLOCK_N, BLOCK_K), order=(1, 0))\n    e_tile_ptr = tl.make_block_ptr(base=E, shape=(M, K), strides=(stride_em, stride_ek), offsets=(pid * BLOCK_M, 0),\n                                   block_shape=(BLOCK_M, BLOCK_K), order=(1, 0))\n\n    acc_e = tl.zeros((BLOCK_M, BLOCK_K), dtype=tl.float32)\n    a = tl.load(a_tile_ptr)\n    for i in range(0, N, BLOCK_N):\n        b = tl.load(b_tile_ptr)\n        o_ab = tl.dot(a, tl.trans(b))\n        c = tl.load(c_tile_ptr)\n        o_ab = o_ab.to(tl.float16)\n        acc_e += tl.dot(o_ab, c)\n        b_tile_ptr = tl.advance(b_tile_ptr, [BLOCK_N, 0])\n        c_tile_ptr = tl.advance(c_tile_ptr, [BLOCK_N, 0])\n\n    acc_e = acc_e.to(tl.float16)\n    tl.store(e_tile_ptr, acc_e)\n\n##################################################################################################################################################  \nimport numpy as np\nimport random\nimport torch \nimport os\nfrom tb_eval.perf.ROCm.performance_utils_pytest import PytestBenchmarker, do_bench_config, save_all_benchmark_results\nfrom typing import Dict\n\n\nresult_gold = {}\n######################################## HELPERS for Eval ########################################\ndef set_seed(seed: int = 42) -> None:\n    \"\"\"\n    Set the random seed for reproducibility across multiple libraries and configure PyTorch for deterministic behavior.\n\n    Args:\n        seed (int): The seed value to set. Default is 42.\n    \"\"\"\n    # Set seed for Python's built-in random module\n    random.seed(seed)\n    # Set seed for NumPy\n    np.random.seed(seed)\n    # Set seed for PyTorch on CPU\n    torch.manual_seed(seed)\n    # Set seed for PyTorch on all GPUs (if available)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    # Ensure deterministic behavior in PyTorch\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set environment variable for hash-based operations\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n######################################## HELPERS for Eval ########################################\n\n\n@pytest.mark.skipif(torch.cuda.get_device_capability()[0] < 9, reason=\"not passed on ampere\")\ndef test_gemm_fusion(request):\n    set_seed()\n\n\n    M, N, K = 4096, 4096, 64\n    BLOCK_M, BLOCK_N, BLOCK_K = 128, 128, 64\n    A = torch.empty((M, K), dtype=torch.float16, device='cuda').normal_(mean=0.1, std=0.2)\n    B = torch.empty((N, K), dtype=torch.float16, device='cuda').normal_(mean=0.1, std=0.2)\n    C = torch.empty((N, K), dtype=torch.float16, device='cuda').normal_(mean=0.1, std=0.2)\n    E = torch.empty((M, K), dtype=torch.float16, device='cuda')\n    ref_out = torch.matmul(torch.matmul(A, B.T), C)\n    num_warps = 4\n    grid = (triton.cdiv(M, BLOCK_M), 1)\n    gemm_fusion_kernel[grid](\n        A, B, C, E, M, N, K,  #\n        A.stride(0), A.stride(1),  #\n        B.stride(0), B.stride(1),  #\n        C.stride(0), C.stride(1),  #\n        E.stride(0), E.stride(1),  #\n        BLOCK_M, BLOCK_N, BLOCK_K,  #\n        num_warps=num_warps)\n\n    result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])\n    ################### save tri_out in result_gold ###################\n    test_case_name = request.node.name\n    sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\") \n    result_gold[sanitized_key_name] = E.clone().detach().cpu()\n    ###################################################################\n\n    torch.testing.assert_close(ref_out, E, atol=1e-2, rtol=1e-2)\n\n\n# --- Python wrapper for the kernel for benchmarking ---\ndef gemm_fusion_triton_wrapper(A_in, B_in, C_in, E_buffer, M_dim, N_dim, K_dim, \n                               block_m_const, block_n_const, num_warps_launch):\n    # K_dim is also block_k_const for this kernel\n    block_k_const = K_dim\n    \n    grid = (triton.cdiv(M_dim, block_m_const), 1) # As per original test\n    \n    gemm_fusion_kernel[grid](\n        A_in, B_in, C_in, E_buffer, M_dim, N_dim, K_dim,\n        A_in.stride(0), A_in.stride(1),\n        B_in.stride(0), B_in.stride(1),\n        C_in.stride(0), C_in.stride(1),\n        E_buffer.stride(0), E_buffer.stride(1),\n        BLOCK_M=block_m_const, BLOCK_N=block_n_const, BLOCK_K=block_k_const,\n        num_warps=num_warps_launch\n    )\n    return E_buffer\n\n# --- Define TFLOPS and GB/s calculators ---\ndef calculate_gemm_fusion_tflops(params: dict, ms: float) -> float:\n    M, N, K = params['M'], params['N'], params['K']\n    # Op1: Intermediate(M,N) = A(M,K) @ B.T(K,N) -> 2 * M * N * K\n    # Op2: Out(M,K) = Intermediate(M,N) @ C(N,K) -> 2 * M * K * N \n    flops = 2 * M * N * K + 2 * M * K * N \n    tflops = flops / (ms / 1000) / 1e12\n    return tflops\n\ndef calculate_gemm_fusion_gbps(params: dict, ms: float) -> float:\n    M, N, K = params['M'], params['N'], params['K']\n    dtype_str = params.get('dtype_str', 'fp16')\n    current_dtype = torch.float16\n    if dtype_str == 'fp32': current_dtype = torch.float32\n    elif dtype_str == 'bf16': current_dtype = torch.bfloat16\n    element_size = torch.tensor([], dtype=current_dtype).element_size()\n\n    bytes_a = M * K * element_size\n    bytes_b = N * K * element_size # B is (N,K)\n    bytes_c_read = N * K * element_size # C is (N,K)\n    bytes_e_write = M * K * element_size # E is (M,K)\n    total_bytes = bytes_a + bytes_b + bytes_c_read + bytes_e_write\n    gbps = total_bytes / (ms / 1000) / 1e9\n    return gbps\n\nOP_NAME_FOR_BENCHMARK = \"gemm_fusion_triton_perf\"\n\n# --- Pytest parametrize for performance testing ---\n# K must be small enough for block_k = K to fit in shared memory for BOTH dots.\n# Constraint approx (fp16): block_m*K + 2*K*block_n + block_m*block_n <= 32768\nGEMM_FUSION_PERF_CONFIGS = []\n# K = 32 (Very safe)\nk_val = 32\nfor M_val in [128, 256, 512, 1024, 2048, 4096]:\n    for N_val in [128, 256, 512, 1024, 2048, 4096]:\n        for bm_val in [32, 64, 128]: # block_m\n            if M_val % bm_val == 0:\n                for bn_val in [32, 64, 128]: # block_n\n                    if (bm_val*k_val + 2*k_val*bn_val + bm_val*bn_val) <= 32768:\n                        GEMM_FUSION_PERF_CONFIGS.append({'M': M_val, 'N': N_val, 'K': k_val, 'bm': bm_val, 'bn': bn_val})\n# K = 64\nk_val = 64\nfor M_val in [128, 256, 512, 1024, 2048]: # Reduced M for larger K\n    for N_val in [128, 256, 512, 1024]:   # Reduced N for larger K\n        for bm_val in [32, 64, 128]:\n            if M_val % bm_val == 0:\n                for bn_val in [32, 64]: # Smaller block_n for K=64\n                    if (bm_val*k_val + 2*k_val*bn_val + bm_val*bn_val) <= 32768:\n                        GEMM_FUSION_PERF_CONFIGS.append({'M': M_val, 'N': N_val, 'K': k_val, 'bm': bm_val, 'bn': bn_val})\n# K = 128 (Requires smaller block_m, block_n)\nk_val = 128\nfor M_val in [64, 128, 256, 512]:\n    for N_val in [64, 128, 256]:\n        for bm_val in [16, 32, 64]:\n            if M_val % bm_val == 0:\n                for bn_val in [16, 32]:\n                    if (bm_val*k_val + 2*k_val*bn_val + bm_val*bn_val) <= 32768:\n                        GEMM_FUSION_PERF_CONFIGS.append({'M': M_val, 'N': N_val, 'K': k_val, 'bm': bm_val, 'bn': bn_val})\n\nunique_configs = []\nseen_configs_str = set()\nfor cfg in GEMM_FUSION_PERF_CONFIGS:\n    cfg_str = f\"M{cfg['M']}_N{cfg['N']}_K{cfg['K']}_bm{cfg['bm']}_bn{cfg['bn']}\"\n    if cfg_str not in seen_configs_str:\n        unique_configs.append(cfg)\n        seen_configs_str.add(cfg_str)\nGEMM_FUSION_PERF_CONFIGS = unique_configs\nprint(f\"Generated {len(GEMM_FUSION_PERF_CONFIGS)} unique performance test configurations for gemm_fusion.\")\n\nGEMM_FUSION_DTYPES_FOR_PERF = ['fp16'] # Original test uses fp16\n# NUM_WARPS_FOR_PERF = [4, 8] # Original test used 4, can parametrize if needed\n\n@pytest.mark.parametrize(\"test_params_dict\", GEMM_FUSION_PERF_CONFIGS)\n@pytest.mark.parametrize(\"dtype_str\", GEMM_FUSION_DTYPES_FOR_PERF)\n# @pytest.mark.skipif(torch.cuda.get_device_capability()[0] < 9, reason=\"not passed on ampere\") # Original skip\ndef test_performance(test_params_dict, dtype_str, request):\n    # Apply capability skip if needed, e.g. for specific dtypes or features\n    # if torch.cuda.get_device_capability()[0] < 8 and dtype_str == 'bf16':\n    #     pytest.skip(\"bf16 requires Ampere+\")\n\n    set_seed()\n    M = test_params_dict['M']\n    N = test_params_dict['N']\n    K = test_params_dict['K']\n    block_m = test_params_dict['bm']\n    block_n = test_params_dict['bn']\n    \n    block_k_const = K # Kernel requires BLOCK_K == K\n\n    if dtype_str == 'fp32': current_dtype = torch.float32\n    elif dtype_str == 'bf16': current_dtype = torch.bfloat16\n    else: current_dtype = torch.float16\n\n    A = torch.randn((M, K), dtype=current_dtype, device='cuda')\n    B = torch.randn((N, K), dtype=current_dtype, device='cuda')\n    C_mat = torch.randn((N, K), dtype=current_dtype, device='cuda')\n    E_buffer = torch.empty((M, K), dtype=current_dtype, device='cuda')\n    \n    num_warps_launch = 4 # As in original test_gemm_fusion\n\n    op_lambda = lambda: gemm_fusion_triton_wrapper(\n        A, B, C_mat, E_buffer, M, N, K,\n        block_m, block_n, num_warps_launch\n    )\n\n    bench_config = do_bench_config(warm_up=25, repetition=100)\n    benchmarker = PytestBenchmarker(op_callable=op_lambda,\n                                    op_name=OP_NAME_FOR_BENCHMARK,\n                                    config=bench_config)\n    current_params_for_logs_and_calc = {\n        \"M\": M, \"N\": N, \"K\": K,\n        \"block_m\": block_m, \"block_n\": block_n, \"block_k\": block_k_const,\n        \"dtype_str\": dtype_str, \"num_warps\": num_warps_launch\n    }\n    \n    perf_result = benchmarker.run_benchmark(current_params_dict=current_params_for_logs_and_calc,\n                                            gbps_calculator=calculate_gemm_fusion_gbps,\n                                            tflops_calculator=calculate_gemm_fusion_tflops)\n\n\n\n######################################## HELPERS for Eval ########################################     \n# --- Pytest hook to save the dictionary at the end of the session ---  \ndef test_save_results():  \n    \"\"\"  \n    Called after whole test run finished, right before returning the exit status to the system.  \n    \"\"\"\n    print('Inside session finish...')\n    if \"_CALL_SUCCESS_\" not in result_gold:\n        result_gold['_CALL_SUCCESS_'] = torch.tensor([[0.0]])\n        \n    OUTPUT_FILENAME = __file__.replace('.','_') + '.pt'\n    print(f\"\\nSaving all y_triton results to {OUTPUT_FILENAME}...\")  \n    # Ensure the directory for the output file exists if it's in a subdirectory  \n    output_dir = os.path.dirname(OUTPUT_FILENAME)  \n    if output_dir and not os.path.exists(output_dir):  \n        os.makedirs(output_dir, exist_ok=True)  \n    torch.save(result_gold, OUTPUT_FILENAME)       \n    print(f\"Successfully saved {len(result_gold)} y_triton tensors to {OUTPUT_FILENAME}.\")  \n\ndef test_save_performance_results():\n    \"\"\"\n    Called after the test_performance function finishes.\n    This is a separate hook to ensure performance results are saved.\n    \"\"\"\n    print('\\nPytest session finishing... Saving benchmark results...')\n\n    output_directory = os.path.join(os.path.dirname(__file__), \"perf\")  # Save in a \"perf\" subdirectory next to the test file\n    os.makedirs(output_directory, exist_ok=True)\n    \n    save_all_benchmark_results(output_directory)\n    print(f\"All benchmark results attempted to save to: {output_directory}\")\n\n\n######################################## HELPERS for Eval ########################################"}, {"file": "test_chained_matmul.py", "target_kernel_name": "chained_matmul_kernel", "instruction": "\nYou are an expert in triton programming language. You will be given the function definition for the `chained_matmul_kernel`. Your task is to complete the kernel code. Only complete the kernel code in the function definition, DONT remove any python imports or helper utils in the instruction/code provided, DONT change/interfere with the provided function definition and parameter list.\n\nThis kernel, `chained_matmul_kernel`,  is designed to perform chained matrix multiplication of the form `(A @ B.T) @ C` on a GPU.\n\n**Your objective is to implement the body of `chained_matmul_kernel`.**\n\nYou must ensure that:\n1.  All arguments received by `chained_matmul_kernel` are kept intact and not modified.\n2. Provide you final code in ```python code block. \nExample:\n```python\n<YOUR-CODE-HERE>\n```\nThe full definition for `chained_matmul_kernel` and relevant helper utilities are provided in the context below. You only need to complete the code for `chained_matmul_kernel` whilst keeping other things intact.\n\n######################################## Imports ########################################\nimport numpy as np\nimport pytest\nimport torch\nfrom numpy.random import RandomState\n\nimport triton\nimport triton.language as tl\n\n######################################## Imports ########################################\n\n\n\n@triton.jit\ndef chained_matmul_kernel(A,  # Pointer to the first input tensor `A`. Expected shape: (m, k). This tensor provides the first operand in the (A @ B.T) operation.\n                            B,  # Pointer to the second input tensor `B`. Expected shape: (n, k). The transpose of this tensor (B.T, shape (k, n)) is used as the second operand in the (A @ B.T) operation.\n                            C,  # Pointer to the third input tensor `C`. Expected shape: (n, k). This tensor is the second operand in the ((A @ B.T) @ C) operation.\n                            out,  # Pointer to the output tensor `out`. Expected shape: (m, k). This tensor will store the result of the chained matrix multiplication (A @ B.T) @ C.\n                            m,    # Integer representing the 'm' dimension. This is the number of rows in matrix `A` and the output matrix `out`.\n                            n,    # Integer representing the 'n' dimension. This is the number of rows in matrices `B` and `C`. It also becomes the shared inner dimension after the A @ B.T operation (i.e., A @ B.T results in an m x n matrix). The kernel iterates over this dimension in `block_n` sized chunks.\n                            k: tl.constexpr,  # Compile-time constant integer for the 'k' dimension. This is the number of columns in matrices `A`, `B`, `C`, and `out`. It's also the shared inner dimension for B.T @ C if C were (k,p) and for A @ B.T if B.T were (k,n). In this kernel, it's the common feature dimension.\n                            block_m: tl.constexpr,  # Compile-time constant integer defining the tile size for the 'm' dimension. Each kernel instance (program) will process `block_m` rows of matrix `A` (and write `block_m` rows to `out`) at a time.\n                            block_n: tl.constexpr,  # Compile-time constant integer defining the tile size for the 'n' dimension. The kernel will iterate through the 'n' dimension in steps of `block_n` when processing matrices `B` and `C`.\n                            block_k: tl.constexpr  # Compile-time constant integer defining the tile size for the 'k' dimension. For this specific kernel, there's a constraint `block_k == k`, meaning the entire 'k' dimension is processed at once within the dot products, rather than being tiled itself for reduction.\n                           ):\n    \"\"\"\n    Brief description of the kernel:\n    This Triton JIT-compiled kernel, `chained_matmul_kernel`, is designed to efficiently compute\n    a chained matrix multiplication of the form `(A @ B.T) @ C` on a GPU.\n    It takes three input matrices: `A` of shape `(m, k)`, `B` of shape `(n, k)`, and `C` of shape `(n, k)`.\n    The transpose of `B` is used in the first multiplication, resulting in an intermediate\n    matrix of shape `(m, n)`. This intermediate result is then multiplied by `C` (after `C` is\n    effectively processed column-wise due to the dot product with the intermediate, or rather,\n    the accumulation logic results in an `(m,k)` output from `(m,n) @ (n,k)` where the second `(n,k)`\n    is `C`). The final output matrix `out` has the shape `(m, k)`.\n    The kernel utilizes a tiled approach for parallel processing. \n\n\n    The user should implement the logic for (A @ B.T) @ C using Triton programming constructs.\n    This typically involves:\n    - Calculating program IDs and offsets for the current block.\n    - Loading tiles of A, B, and C.\n    - Performing the dot products and accumulations in a loop over the 'n' dimension.\n    - Handling boundary conditions carefully with masks.\n    - Storing the resulting tile to the output tensor `out`\n\n    \"\"\"\n    # Your code here\n\n", "label": "######################################## Imports ######################################## \nimport numpy as np\nimport pytest\nimport torch\nfrom numpy.random import RandomState\n\nimport triton\nimport triton.language as tl\n\n######################################## Imports ######################################## \n\n\n@triton.jit\ndef chained_matmul_kernel(A,  # shape: (m, k)\n                            B,  # shape: (n, k)\n                            C,  # shape: (n, k)\n                            out,  # shape: (m, k)\n                            m, n, k: tl.constexpr,  #\n                            block_m: tl.constexpr, block_n: tl.constexpr, block_k: tl.constexpr):\n\n    tl.static_assert(block_k == k, f\"expected block_k == k but got {block_k} != {k}\")\n\n    block_ix = tl.program_id(0)\n    a_tile = (block_ix * block_m + tl.arange(0, block_m))[:, None] * block_k \\\n        + tl.arange(0, block_k)[None, :]\n\n    a = tl.load(A + a_tile, mask=a_tile < m * k, other=0.0)\n\n    acc = tl.zeros([block_m, block_k], dtype=tl.float32)\n\n    for loop_block_start in range(0, n, block_n):\n        bc_tile = (loop_block_start + tl.arange(0, block_n))[:, None] * block_k \\\n            + tl.arange(0, block_k)[None, :]\n        b = tl.load(B + bc_tile, mask=bc_tile < n * k, other=0.0)\n\n        intermediate = tl.dot(a, tl.trans(b))\n        intermediate_mask = ((loop_block_start + tl.arange(0, block_n)) < n)[None, :] \\\n            * (tl.arange(0, block_m) < m)[:, None]\n\n        intermediate = tl.where(intermediate_mask, intermediate, 0.0)\n\n        c = tl.load(C + bc_tile, mask=bc_tile < n * k)\n\n        acc += tl.dot(intermediate.to(A.dtype.element_ty), c)\n\n    tl.store(out + a_tile, acc.to(A.dtype.element_ty), mask=a_tile < m * k)\n\n\n##################################################################################################################################################  \n\nimport numpy as np\nimport random\nimport torch \nimport os\nimport pytest\nfrom numpy.random import RandomState\nfrom tb_eval.perf.ROCm.performance_utils_pytest import PytestBenchmarker, do_bench_config, save_all_benchmark_results\nfrom typing import Dict\n\nresult_gold = {}\n\n######################################## HELPERS for Eval ######################################## \ndef set_seed(seed: int = 42) -> None:\n    \"\"\"\n    Set the random seed for reproducibility across multiple libraries and configure PyTorch for deterministic behavior.\n\n    Args:\n        seed (int): The seed value to set. Default is 42.\n    \"\"\"\n    # Set seed for Python's built-in random module\n    random.seed(seed)\n    # Set seed for NumPy\n    np.random.seed(seed)\n    # Set seed for PyTorch on CPU\n    torch.manual_seed(seed)\n    # Set seed for PyTorch on all GPUs (if available)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    # Ensure deterministic behavior in PyTorch\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set environment variable for hash-based operations\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n######################################## HELPERS for Eval ######################################## \n\n\ndef chained_matmul_reference(a, b, c):\n    intermediate = torch.einsum('MK,NK->MN', a, b)\n    return torch.einsum('MN,NK->MK', intermediate, c)\n\n\ndef test_chained_matmul(request, device='cuda'):\n    # Regression test for issue #1601\n    set_seed()\n\n\n    m, n, k = 32, 64, 128\n    block_m, block_n, block_k = 16, 32, k\n\n    grid = (triton.cdiv(m, block_m), )\n    a = torch.randint(low=0, high=2, size=(m, k), dtype=torch.float16, device=device)\n    b = torch.randint(low=0, high=2, size=(n, k), dtype=torch.float16, device=device)\n    c = torch.randint_like(b, low=0, high=2)\n    triton_result = torch.zeros_like(a)\n\n    torch_result = chained_matmul_reference(a, b, c)\n    chained_matmul_kernel[grid](\n        a, b, c, triton_result, m, n, k,  #\n        block_m=block_m, block_n=block_n, block_k=block_k)\n\n    result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])\n    ################### save tri_out in result_gold ###################\n    test_case_name = request.node.name\n    sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\")\n    result_gold[sanitized_key_name] = triton_result.clone().detach().cpu()\n    ################################################################### \n    assert (torch_result == triton_result).all()\n\n# --- Python wrapper for the kernel ---\ndef chained_matmul_triton_wrapper(A_in, B_in, C_in, out_buffer, block_m, block_n):\n    m, k_a = A_in.shape\n    n, k_b = B_in.shape\n    _n_c, k_c = C_in.shape # C is also (n,k)\n    assert k_a == k_b and k_a == k_c, \"K dimensions must match for A, B, C\"\n    \n    # Kernel's k is a constexpr fixed to k_a (full K dimension)\n    # Kernel's block_k is also fixed to k_a\n    \n    grid = (triton.cdiv(m, block_m),) # Grid is 1D, iterating over M blocks\n    \n    chained_matmul_kernel[grid](\n        A_in, B_in, C_in, out_buffer,\n        m, n, k_a, # runtime m, n, k\n        block_m=block_m, block_n=block_n, block_k=k_a # constexpr block_m, block_n, block_k=k\n        # num_warps not in kernel signature, would be for autotune\n    )\n    return out_buffer\n\n# --- Define TFLOPS and GB/s calculators ---\ndef calculate_chained_matmul_tflops(params: dict, ms: float) -> float:\n    m, n, k = params['M'], params['N'], params['K']\n    # Op1: Intermediate(M,N) = A(M,K) @ B.T(K,N) -> 2 * M * N * K FLOPs\n    # Op2: Out(M,K) = Intermediate(M,N) @ C(N,K) -> 2 * M * K * N FLOPs (Note: C is (N,K))\n    # Total FLOPs = 4 * M * N * K\n    flops = 2 * m * n * k + 2 * m * k * n \n    tflops = flops / (ms / 1000) / 1e12\n    return tflops\n\ndef calculate_chained_matmul_gbps(params: dict, ms: float) -> float:\n    m, n, k = params['M'], params['N'], params['K']\n    dtype_str = params.get('dtype_str', 'fp16')\n    current_dtype = torch.float16\n    if dtype_str == 'fp32': current_dtype = torch.float32\n    elif dtype_str == 'bf16': current_dtype = torch.bfloat16\n    element_size = torch.tensor([], dtype=current_dtype).element_size()\n    bytes_a = m * k * element_size\n    bytes_b = n * k * element_size\n    bytes_c_read = n * k * element_size\n    bytes_out_write = m * k * element_size\n    total_bytes = bytes_a + bytes_b + bytes_c_read + bytes_out_write\n    gbps = total_bytes / (ms / 1000) / 1e9\n    return gbps\n\nOP_NAME_FOR_BENCHMARK = \"chained_matmul_triton_perf\"\n\n# --- REVISED Pytest parametrize for performance testing ---\n# K must be small enough for block_k = K to fit in shared memory.\n# Constraint: K * (block_m + block_n) <= 32768 (for fp16, 64KB shared mem limit)\n\nCM_PERF_TEST_CONFIGS = []\n# Shared memory constraint (fp16): block_m*K + 2*K*block_n + block_m*block_n <= 32768\n\n# K = 64\nk_val = 64\nfor M_val in [64, 128, 256]:\n    for N_val in [64, 128, 256]: # N is the total dimension, looped over by block_n\n        for bm_val in [16, 32, 64]:\n            if M_val % bm_val == 0:\n                for bn_val in [16, 32]: # Keep block_n small\n                    if (bm_val*k_val + 2*k_val*bn_val + bm_val*bn_val) <= 32768:\n                        CM_PERF_TEST_CONFIGS.append({'M': M_val, 'N': N_val, 'K': k_val, 'bm': bm_val, 'bn': bn_val})\n                # Try bn_val = 64 if bm_val is small\n                if bm_val <= 32:\n                    bn_val = 64\n                    if (bm_val*k_val + 2*k_val*bn_val + bm_val*bn_val) <= 32768:\n                        CM_PERF_TEST_CONFIGS.append({'M': M_val, 'N': N_val, 'K': k_val, 'bm': bm_val, 'bn': bn_val})\n\n\n# K = 128\nk_val = 128\nfor M_val in [64, 128, 256]:\n    for N_val in [32, 64, 128]: \n        for bm_val in [16, 32]: # Keep block_m smaller for larger K\n            if M_val % bm_val == 0:\n                for bn_val in [16, 32]: # Keep block_n small\n                    if (bm_val*k_val + 2*k_val*bn_val + bm_val*bn_val) <= 32768:\n                        CM_PERF_TEST_CONFIGS.append({'M': M_val, 'N': N_val, 'K': k_val, 'bm': bm_val, 'bn': bn_val})\n        # Try bm_val = 64 if bn_val is very small\n        if M_val % 64 == 0:\n            bm_val = 64\n            bn_val = 16 \n            if (bm_val*k_val + 2*k_val*bn_val + bm_val*bn_val) <= 32768:\n                 CM_PERF_TEST_CONFIGS.append({'M': M_val, 'N': N_val, 'K': k_val, 'bm': bm_val, 'bn': bn_val})\n\n\n# K = 256 (Requires even smaller block_m, block_n)\nk_val = 256\nfor M_val in [32, 64, 128]: \n    for N_val in [16, 32, 64]:  \n        for bm_val in [16, 32]: \n            if M_val % bm_val == 0:\n                for bn_val in [16]: # block_n must be very small\n                    if (bm_val*k_val + 2*k_val*bn_val + bm_val*bn_val) <= 32768:\n                        CM_PERF_TEST_CONFIGS.append({'M': M_val, 'N': N_val, 'K': k_val, 'bm': bm_val, 'bn': bn_val})\n                # Try bn_val = 32 if bm_val is 16\n                if bm_val == 16:\n                    bn_val = 32\n                    if (bm_val*k_val + 2*k_val*bn_val + bm_val*bn_val) <= 32768: # 16*256 + 2*256*32 + 16*32 = 4096 + 16384 + 512 = 20992 (OK)\n                        CM_PERF_TEST_CONFIGS.append({'M': M_val, 'N': N_val, 'K': k_val, 'bm': bm_val, 'bn': bn_val})\n\n\n# K = 512 (Extremely restrictive for this kernel design) - Likely only 16x16 blocks will work\nk_val = 512\nfor M_val in [16, 32, 64]: \n    for N_val in [16, 32]: \n        for bm_val in [16]: \n            if M_val % bm_val == 0:\n                for bn_val in [16]: \n                    # 16*512 + 2*512*16 + 16*16 = 8192 + 16384 + 256 = 24832 (OK)\n                    if (bm_val*k_val + 2*k_val*bn_val + bm_val*bn_val) <= 32768:\n                        CM_PERF_TEST_CONFIGS.append({'M': M_val, 'N': N_val, 'K': k_val, 'bm': bm_val, 'bn': bn_val})\n\n\n# Remove duplicates\nunique_configs = []\nseen_configs_str = set()\nfor cfg in CM_PERF_TEST_CONFIGS:\n    # Create a string representation for checking uniqueness easily\n    cfg_str = f\"M{cfg['M']}_N{cfg['N']}_K{cfg['K']}_bm{cfg['bm']}_bn{cfg['bn']}\"\n    if cfg_str not in seen_configs_str:\n        unique_configs.append(cfg)\n        seen_configs_str.add(cfg_str)\nCM_PERF_TEST_CONFIGS = unique_configs\n\nprint(f\"Generated {len(CM_PERF_TEST_CONFIGS)} unique performance test configurations for chained_matmul.\")\n\n# Only fp16 for now to reduce matrix and focus on shared mem issue\nCM_DTYPES_FOR_PERF = ['fp16']\n# CM_DTYPES_FOR_PERF = ['fp16', 'fp32'] # Add fp32 later if fp16 works\n\n\n@pytest.mark.parametrize(\"test_params_dict\", CM_PERF_TEST_CONFIGS)\n@pytest.mark.parametrize(\"dtype_str\", CM_DTYPES_FOR_PERF)\ndef test_performance(test_params_dict, dtype_str, request):\n    # ... (rest of the test_chained_matmul_performance function remains the same as in the previous response)\n    set_seed()\n    m = test_params_dict['M']\n    n = test_params_dict['N']\n    k = test_params_dict['K']\n    block_m = test_params_dict['bm']\n    block_n = test_params_dict['bn']\n    \n    block_k_const = k \n\n    # This skip logic might be redundant if CM_PERF_TEST_CONFIGS is well-filtered\n    # element_size_bytes = 4 if dtype_str == 'fp32' else 2\n    # shared_mem_limit_elements = 65536 // element_size_bytes\n    # estimated_elements_needed = block_m * k + 2 * k * block_n + block_m * block_n\n    # if estimated_elements_needed > shared_mem_limit_elements:\n    #     pytest.skip(f\"Skipping M{m}N{n}K{k} bm{block_m}bn{block_n} dtype {dtype_str} due to estimated shared memory: {estimated_elements_needed*element_size_bytes} vs {65536}\")\n\n\n    if dtype_str == 'fp32': current_dtype = torch.float32\n    elif dtype_str == 'bf16': current_dtype = torch.bfloat16\n    else: current_dtype = torch.float16\n\n    a = torch.randn((m, k), dtype=current_dtype, device='cuda')\n    b = torch.randn((n, k), dtype=current_dtype, device='cuda') \n    c_mat = torch.randn((n, k), dtype=current_dtype, device='cuda')\n    triton_result_buffer = torch.empty((m, k), dtype=current_dtype, device='cuda')\n\n    op_lambda = lambda: chained_matmul_triton_wrapper(\n        a, b, c_mat, triton_result_buffer,\n        block_m, block_n\n    )\n\n    bench_config = do_bench_config(warm_up=25, repetition=100)\n    benchmarker = PytestBenchmarker(op_callable=op_lambda,\n                                    op_name=OP_NAME_FOR_BENCHMARK,\n                                    config=bench_config)\n\n    current_params_for_logs_and_calc = {\n        \"M\": m, \"N\": n, \"K\": k,\n        \"block_m\": block_m, \"block_n\": block_n, \"block_k\": block_k_const,\n        \"dtype_str\": dtype_str\n    }\n\n    benchmarker.run_benchmark(current_params_dict=current_params_for_logs_and_calc,\n                              gbps_calculator=calculate_chained_matmul_gbps,\n                              tflops_calculator=calculate_chained_matmul_tflops)\n######################################## HELPERS for Eval ########################################     \n# --- Pytest hook to save the dictionary at the end of the session ---  \ndef test_save_results():  \n    \"\"\"  \n    Called after whole test run finished, right before returning the exit status to the system.  \n    \"\"\"\n    print('Inside session finish...')\n    if \"_CALL_SUCCESS_\" not in result_gold:\n        result_gold['_CALL_SUCCESS_'] = torch.tensor([[0.0]])\n    OUTPUT_FILENAME = __file__.replace('.','_') + '.pt'\n    print(f\"\\nSaving all y_triton results to {OUTPUT_FILENAME}...\")  \n    # Ensure the directory for the output file exists if it's in a subdirectory  \n    output_dir = os.path.dirname(OUTPUT_FILENAME)  \n    if output_dir and not os.path.exists(output_dir):  \n        os.makedirs(output_dir, exist_ok=True)  \n    torch.save(result_gold, OUTPUT_FILENAME)       \n    print(f\"Successfully saved {len(result_gold)} y_triton tensors to {OUTPUT_FILENAME}.\")  \n\ndef test_save_performance_results():\n    \"\"\"\n    Called after the test_performance function finishes.\n    This is a separate hook to ensure performance results are saved.\n    \"\"\"\n    print('\\nPytest session finishing... Saving benchmark results...')\n\n    output_directory = os.path.join(os.path.dirname(__file__), \"perf\")  # Save in a \"perf\" subdirectory next to the test file\n    os.makedirs(output_directory, exist_ok=True)\n    \n    save_all_benchmark_results(output_directory)\n    print(f\"All benchmark results attempted to save to: {output_directory}\")\n\n######################################## HELPERS for Eval ########################################"}, {"file": "test_batched_vecmat.py", "target_kernel_name": "batched_vecmat", "instruction": "\nYou are an expert in triton programming language. You will be given the function definition for the `batched_vecmat`. Your task is to complete the kernel code. Only complete the kernel code in the function definition, DONT remove any python imports or helper utils in the instruction/code provided, DONT change/interfere with the provided function definition and parameter list.\n\nThis kernel, `batched_vecmat`,  is designed to perform batched element-wise multiplication and sum operation.\n\n**Your objective is to implement the body of `batched_vecmat`.**\n\nYou must ensure that:\n1.  All arguments received by `batched_vecmat` are kept intact and not modified.\n2. Provide you final code in ```python code block. \nExample:\n```python\n<YOUR-CODE-HERE>\n```\nThe full definition for `batched_vecmat` and relevant helper utilities are provided in the context below. You only need to complete the code for `batched_vecmat` whilst keeping other things intact. DONT remove Imports and HELPER utils.\n\n######################################## Imports ########################################\nimport numpy as np\nimport pytest\nimport torch\nfrom numpy.random import RandomState\n\nimport triton\nimport triton.language as tl\n\n######################################## Imports ########################################\n\n@triton.jit\ndef batched_vecmat(\n    A,\n    B,\n    dim_m, dim_n, dim_k,\n    output,\n    block_m: tl.constexpr, block_n: tl.constexpr, block_k: tl.constexpr\n):\n    \"\"\"\n    Performs a batched element-wise multiplication and sum operation.\n    Effectively, for each m in dim_m, it computes the element-wise product of\n    A[m, :] with each row of B[m, :, :], and sums the results.\n    This can be expressed as: output[m, n] = sum_k (A[m, k] * B[m, n, k]).\n    This is equivalent to `torch.sum(A.unsqueeze(1) * B, dim=2)`.\n\n    Parameters\n    ----------\n    A\n        Input tensor of shape [dim_m, dim_k].\n        Interpreted as a batch of `dim_m` vectors, each of size `dim_k`.\n    B\n        Input tensor of shape [dim_m, dim_n, dim_k].\n        Interpreted as a batch of `dim_m` groups of vectors. Each group `B[i, :, :]`\n        contains `dim_n` vectors, each of size `dim_k`.\n    dim_m\n        The size of the first dimension of A and B (batch dimension).\n    dim_n\n        The size of the second dimension of B and the output tensor.\n        Represents the number of vectors in each batch entry of B.\n    dim_k\n        The size of the last dimension of A and B (the dimension over which the sum is performed).\n    output\n        Output tensor of shape [dim_m, dim_n] where the results are stored.\n    block_m\n        tl.constexpr: Tiling size for the m dimension.\n    block_n\n        tl.constexpr: Tiling size for the n dimension.\n    block_k\n        tl.constexpr: Tiling size for the k dimension (reduction dimension).\n    \"\"\"\n    # Your code here\n\n", "label": "######################################## Imports ######################################## \nimport numpy as np\nimport pytest\nimport torch\nfrom numpy.random import RandomState\n\nimport triton\nimport triton.language as tl\n\n######################################## Imports ######################################## \n\n@triton.jit\ndef batched_vecmat(\n        # inputs\n        A,  # shape: [dim_m, dim_k]\n        B,  # shape: [dim_m, dim_n, dim_k]\n        # dimensions\n    dim_m, dim_n, dim_k,\n        # outputs\n        output,\n        # block information\n        block_m: tl.constexpr, block_n: tl.constexpr, block_k: tl.constexpr):\n    m_index = tl.program_id(0)\n    n_index = tl.program_id(1)\n    # Output tile\n    output_tile = (m_index * block_m + tl.arange(0, block_m))[:, None] * dim_n \\\n        + (n_index * block_n + tl.arange(0, block_n))[None, :]\n\n    vecmat = tl.zeros([block_m, block_n], dtype=A.dtype.element_ty)\n    k_blocks = dim_k // block_k\n    for k_index in range(k_blocks):\n        # Load A tile\n        a_tile = (m_index * block_m + tl.arange(0, block_m))[:, None] * dim_k \\\n            + (k_index * block_k + tl.arange(0, block_k))[None, :]\n        a = tl.load(A + a_tile)\n\n        # Load B tile, transposed to [n, m, k] in order to broadcast A on a\n        # leading dimension.\n        b_tile = (m_index * block_m + tl.arange(0, block_m))[None, :, None] * dim_n * dim_k \\\n            + (n_index * block_n + tl.arange(0, block_n))[:, None, None] * dim_k \\\n            + (k_index * block_k + tl.arange(0, block_k))[None, None, :]\n        b = tl.load(B + b_tile)\n\n        expanded_a, _ = tl.broadcast(a, b)\n        vecmat += tl.trans(tl.sum(expanded_a * b, axis=2))\n\n    tl.store(output + output_tile, vecmat)\n\n\n##################################################################################################################################################  \n\nimport numpy as np\nimport random\nimport torch \nimport os\nimport pytest\nfrom numpy.random import RandomState\n\nfrom tb_eval.perf.ROCm.performance_utils_pytest import PytestBenchmarker, do_bench_config, save_all_benchmark_results\nfrom typing import Dict\n\nresult_gold = {}\n\n######################################## HELPERS for Eval ######################################## \ndef set_seed(seed: int = 42) -> None:\n    \"\"\"\n    Set the random seed for reproducibility across multiple libraries and configure PyTorch for deterministic behavior.\n\n    Args:\n        seed (int): The seed value to set. Default is 42.\n    \"\"\"\n    # Set seed for Python's built-in random module\n    random.seed(seed)\n    # Set seed for NumPy\n    np.random.seed(seed)\n    # Set seed for PyTorch on CPU\n    torch.manual_seed(seed)\n    # Set seed for PyTorch on all GPUs (if available)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    # Ensure deterministic behavior in PyTorch\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set environment variable for hash-based operations\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n######################################## HELPERS for Eval ######################################## \n\n  \ndef test_vecmat(request, device='cuda'):  \n    \"\"\"  \n    Test the batched vector-matrix multiplication kernel.  \n  \n    Args:  \n        device: The device (e.g., 'cuda' or 'cpu') on which the test is executed.  \n        request: Pytest request object used to retrieve the test case name.  \n    \"\"\"  \n    set_seed()  \n  \n    M, N, K = 128, 128, 128  \n    block_m, block_n, block_k = 16, 32, 64  \n  \n    rs = RandomState(17)  \n    A_vec = rs.randint(0, 4, (M, K)).astype('float32')  \n    B_vec = rs.randint(0, 4, (M, N, K)).astype('float32')  \n    A = A_vec  \n    B = B_vec  \n  \n    A_tri = torch.tensor(A, device=device)  \n    B_tri = torch.tensor(B, device=device)  \n    C_tri = torch.zeros((M, N), dtype=torch.float32, device=device)  \n  \n    grid = (M // block_m, N // block_n)  \n  \n    # This is where the actual kernel would run and populate C_tri  \n    # If using the MockKernel above, C_tri will remain zeros unless populated for testing.  \n    # For the purpose of demonstrating saving, we'll assume C_tri is populated by the kernel.  \n    # To make the assert_allclose pass without a real kernel, we can compute C_tri using torch:  \n    if isinstance(batched_vecmat, MockKernel): # If using the mock  \n        A_expanded_torch = A_tri[:, None, :] # (M, 1, K)  \n        # B_tri is (M, N, K)  \n        # Element-wise product and sum over K  \n        # This is what the kernel is supposed to compute  \n        C_tri_computed_by_torch = torch.sum(A_expanded_torch * B_tri, dim=2) # (M, N)  \n        C_tri.copy_(C_tri_computed_by_torch) # Populate C_tri as the kernel would  \n    else: # If using the real kernel  \n        batched_vecmat[grid](  \n            A_tri, B_tri, M, N, K, C_tri,  #  \n            block_m=block_m, block_n=block_n, block_k=block_k,  #  \n            num_warps=4, num_stages=1)  \n  \n  \n    A_expanded_np = A[:, np.newaxis, :] # (M, 1, K)  \n    A_broadcasted_np = np.broadcast_to(A_expanded_np, (M, N, K)) # (M, N, K)  \n    AB_np = A_broadcasted_np * B # B is (M, N, K)  \n    C_ref = np.sum(AB_np, axis=2) # (M, N)  \n  \n    ################### save C_ref and C_tri as torch tensors in result_gold ###################  \n    test_case_name = request.node.name  \n    sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\")  \n  \n    # Convert C_ref (numpy array) to a torch tensor on CPU  \n    # C_ref is float32 because A_vec and B_vec are float32.  \n    C_ref_torch = torch.tensor(C_ref, dtype=torch.float32, device='cpu')  \n  \n    # Store C_ref_torch and C_tri (on CPU) in result_gold  \n    # C_tri is already torch.float32 as defined.  \n    result_gold[sanitized_key_name + \"_ref\"] = C_ref_torch  \n    result_gold[sanitized_key_name + \"_tri\"] = C_tri.cpu() # Ensure C_tri is on CPU for saving  \n    #########################################################################################  \n  \n    np.testing.assert_allclose(C_ref, C_tri.cpu().numpy(), rtol=0.01, atol=1e-3)  \n  \n\n\ndef test_vecmat(request, device='cuda'):\n    \"\"\"\n    Test the batched vector-matrix multiplication kernel.\n\n    Args:\n        device: The device (e.g., 'cuda' or 'cpu') on which the test is executed.\n        request: Pytest request object used to retrieve the test case name.\n    \"\"\"\n    set_seed()\n\n    M, N, K = 128, 128, 128\n    block_m, block_n, block_k = 16, 32, 64\n\n    rs = RandomState(17)\n    A_vec = rs.randint(0, 4, (M, K)).astype('float32')\n    B_vec = rs.randint(0, 4, (M, N, K)).astype('float32')\n    A = A_vec\n    B = B_vec\n\n    A_tri = torch.tensor(A, device=device)\n    B_tri = torch.tensor(B, device=device)\n    C_tri = torch.zeros((M, N), dtype=torch.float32, device=device)\n\n    grid = (M // block_m, N // block_n)\n\n    batched_vecmat[grid](\n        A_tri, B_tri, M, N, K, C_tri,  #\n        block_m=block_m, block_n=block_n, block_k=block_k,  #\n        num_warps=4, num_stages=1)\n\n    A_expanded = A[:, np.newaxis, :]\n    A_broadcasted = np.broadcast_to(A_expanded, (M, N, K))\n    AB = A_broadcasted * B\n    C_ref = np.sum(AB, axis=2)\n\n    ################### save tri_out in result_gold ###################\n    # Convert C_ref (numpy array) to a torch tensor on CPU  \n    # C_ref is float32 because A_vec and B_vec are float32.  \n\n    result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])\n    \n    ## triton_result is assumed to be torch tensor not numpy ndarray\n    test_case_name = request.node.name\n    sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\")\n    result_gold[sanitized_key_name] = C_tri.cpu()\n    ################################################################### \n    result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])\n    np.testing.assert_allclose(C_ref, C_tri.cpu().numpy(), rtol=0.01, atol=1e-3)\n\n# --- Define TFLOPS and GB/s calculators for Batched VecMat ---\ndef calculate_batched_vecmat_tflops(params: dict, ms: float) -> float:\n    M, N, K = params['M'], params['N'], params['K']\n    # Operation: C[m,n] = sum_k A[m,k] * B[m,n,k]\n    # For each (m,n) pair, K multiplications and K-1 additions. Approx 2*K FLOPs.\n    # There are M*N such pairs.\n    # Total FLOPs = M * N * (2 * K)\n    flops = 2 * M * N * K\n    tflops = flops / (ms / 1000) / 1e12\n    return tflops\n\ndef calculate_batched_vecmat_gbps(params: dict, ms: float) -> float:\n    M, N, K = params['M'], params['N'], params['K']\n    dtype_str = params.get('dtype_str', 'fp32') # Original test uses float32\n\n    if dtype_str == 'fp16': current_dtype = torch.float16\n    elif dtype_str == 'bf16': current_dtype = torch.bfloat16\n    else: current_dtype = torch.float32 # Default to float32 as per original test\n    element_size = torch.tensor([], dtype=current_dtype).element_size()\n\n    # Read A (M,K)\n    # Read B (M,N,K)\n    # Write C (M,N)\n    bytes_a = M * K * element_size\n    bytes_b = M * N * K * element_size\n    bytes_c = M * N * element_size\n    total_bytes = bytes_a + bytes_b + bytes_c\n    gbps = total_bytes / (ms / 1000) / 1e9\n    return gbps\n\n# --- Python wrapper for the batched_vecmat kernel ---\ndef batched_vecmat_triton_wrapper(A_in, B_in, block_m, block_n, block_k):\n    dim_m, dim_k_a = A_in.shape\n    dim_m_b, dim_n, dim_k_b = B_in.shape\n    assert dim_m == dim_m_b, \"M dimension must match for A and B\"\n    assert dim_k_a == dim_k_b, \"K dimension must match for A and B\"\n    \n    output_tensor = torch.zeros((dim_m, dim_n), dtype=A_in.dtype, device=A_in.device)\n    \n    grid = (triton.cdiv(dim_m, block_m), triton.cdiv(dim_n, block_n))\n    \n    batched_vecmat[grid](\n        A_in, B_in,\n        dim_m, dim_n, dim_k_a, # Pass runtime dimensions\n        output_tensor,\n        # Strides can be passed if non-contiguous, e.g., A_in.stride(0), A_in.stride(1), ...\n        # For now, assume contiguous and Triton handles it.\n        block_m=block_m, block_n=block_n, block_k=block_k # Pass constexpr block sizes\n        # num_warps, num_stages are not in this kernel's signature directly (would be via @triton.autotune if used)\n    )\n    return output_tensor\n\n# --- Name for the benchmark output JSON file ---\nOP_NAME_FOR_BENCHMARK = \"batched_vecmat_triton_perf\"\n\n# --- Pytest parametrize for performance testing ---\n# Define shapes and block sizes for performance testing\nBV_SHAPES_FOR_PERF = [\n    # M,  N,   K\n    (128, 128, 128),\n    (256, 256, 256),\n    (512, 64,  1024), # More K\n    (64,  512, 256),  # More N\n    (1024, 32, 512),  # Large M, smaller N\n]\n# Block sizes are constexpr. For a fair benchmark of *this specific kernel version*,\n# we should use fixed block sizes or create different test entries for different block configs.\n# The original test_vecmat uses fixed block_m=16, block_n=32, block_k=64.\n# Let's use these, and optionally add more block configs.\nBV_BLOCK_CONFIGS_FOR_PERF = [\n    # block_m, block_n, block_k\n    (16, 32, 64),\n    (32, 32, 128), # Example alternative block config\n    (16, 64, 64),\n]\nBV_DTYPES_FOR_PERF = ['fp32', 'fp16'] # Original uses fp32, add fp16 for variety\n\n@pytest.mark.parametrize(\"M, N, K\", BV_SHAPES_FOR_PERF)\n@pytest.mark.parametrize(\"block_m, block_n, block_k\", BV_BLOCK_CONFIGS_FOR_PERF)\n@pytest.mark.parametrize(\"dtype_str\", BV_DTYPES_FOR_PERF)\ndef test_performance(M, N, K, block_m, block_n, block_k, dtype_str, request):\n    set_seed()\n\n    # Ensure M is divisible by block_m, N by block_n for simpler grid/kernel.\n    # The kernel itself should handle arbitrary M,N with masking.\n    # For performance, often aligned sizes are tested, but let's assume kernel handles it.\n    if M % block_m != 0 or N % block_n != 0:\n        pytest.skip(f\"Skipping M={M},N={N} with block_m={block_m},block_n={block_n} due to non-divisibility for perf simplicity.\")\n\n    if dtype_str == 'fp16': current_dtype = torch.float16\n    elif dtype_str == 'bf16': current_dtype = torch.bfloat16 # Not in original, but for consistency\n    else: current_dtype = torch.float32\n\n    # Input tensors\n    # Original test used rs.randint(0,4,...).astype('float32')\n    # For perf, torch.randn is more common.\n    A_tri = torch.randn((M, K), dtype=current_dtype, device='cuda')\n    B_tri = torch.randn((M, N, K), dtype=current_dtype, device='cuda')\n    # C_tri (output) will be created by the wrapper\n\n    # --- Create op_lambda for benchmarking ---\n    op_lambda = lambda: batched_vecmat_triton_wrapper(A_tri, B_tri, block_m, block_n, block_k)\n\n    # --- Benchmarking ---\n    bench_config = do_bench_config(warm_up=25, repetition=100)\n    benchmarker = PytestBenchmarker(op_callable=op_lambda,\n                                    op_name=OP_NAME_FOR_BENCHMARK,\n                                    config=bench_config)\n\n    current_params_for_logs_and_calc = {\n        \"M\": M, \"N\": N, \"K\": K,\n        \"block_m\": block_m, \"block_n\": block_n, \"block_k\": block_k,\n        \"dtype_str\": dtype_str\n    }\n\n    benchmarker.run_benchmark(current_params_dict=current_params_for_logs_and_calc,\n                              gbps_calculator=calculate_batched_vecmat_gbps,\n                              tflops_calculator=calculate_batched_vecmat_tflops)\n    \n######################################## HELPERS for Eval ########################################     \n# --- Pytest hook to save the dictionary at the end of the session ---  \ndef test_save_results():  \n    \"\"\"  \n    Called after whole test run finished, right before returning the exit status to the system.  \n    \"\"\"\n    print('Inside session finish...')\n    if \"_CALL_SUCCESS_\" not in result_gold:\n        result_gold['_CALL_SUCCESS_'] = torch.tensor([[0.0]])\n    OUTPUT_FILENAME = __file__.replace('.','_') + '.pt'\n    print(f\"\\nSaving all y_triton results to {OUTPUT_FILENAME}...\")  \n    # Ensure the directory for the output file exists if it's in a subdirectory  \n    output_dir = os.path.dirname(OUTPUT_FILENAME)  \n    if output_dir and not os.path.exists(output_dir):  \n        os.makedirs(output_dir, exist_ok=True)  \n    torch.save(result_gold, OUTPUT_FILENAME)       \n    print(f\"Successfully saved {len(result_gold)} y_triton tensors to {OUTPUT_FILENAME}.\")  \n\ndef test_save_performance_results():\n    \"\"\"\n    Called after the test_performance function finishes.\n    This is a separate hook to ensure performance results are saved.\n    \"\"\"\n    print('\\nPytest session finishing... Saving benchmark results...')\n\n    output_directory = os.path.join(os.path.dirname(__file__), \"perf\")  # Save in a \"perf\" subdirectory next to the test file\n    os.makedirs(output_directory, exist_ok=True)\n    \n    save_all_benchmark_results(output_directory)\n    print(f\"All benchmark results attempted to save to: {output_directory}\")\n\n######################################## HELPERS for Eval ########################################"}, {"file": "test_reverse_range.py", "target_kernel_name": "reverse_range", "instruction": "\nYou are an expert in triton programming language. You will be given the function definition for the `reverse_range`. Your task is to complete the kernel code. Only complete the kernel code in the function definition, DONT remove any python imports or helper utils in the instruction/code provided, DONT change/interfere with the provided function definition and parameter list.\n\nThis kernel, `reverse_range`,  is designed to reverse a specific 512-element segment from an input tensor and stores it into an output tensor.\n\n**Your objective is to implement the body of `reverse_range`.**\n\nYou must ensure that:\n1.  All arguments received by `reverse_range` are kept intact and not modified.\n2. Provide you final code in ```python code block. \nExample:\n```python\n<YOUR-CODE-HERE>\n```\nThe full definition for `reverse_range` and relevant helper utilities are provided in the context below. You only need to complete the code for `reverse_range` whilst keeping other things intact. DONT remove Imports and HELPER utils.\n\n######################################## Imports ######################################## \nimport numpy as np\nimport pytest\nimport torch\nfrom numpy.random import RandomState\n\nimport triton\nimport triton.language as tl\n######################################## Imports ######################################## \n\n@triton.jit\ndef reverse_range(in_ptr, out_ptr):\n    \"\"\"\n    Reverses a specific 512-element segment from an input tensor and stores it\n    into an output tensor.\n\n    This kernel operates on fixed-size blocks of 512 elements.\n    It performs the following operation for each element `i` in the range `[0, 511]`:\n    `out_ptr[i] = in_ptr[512 - i]`\n\n    This means the kernel reads elements from `in_ptr + 1` up to `in_ptr + 512`\n    (inclusive) and writes them in reversed order to `out_ptr + 0` up to\n    `out_ptr + 511` (inclusive).\n\n    Parameters\n    ----------\n    in_ptr : tl.pointer_type\n        A pointer to the input tensor. The kernel reads a block of 512 elements\n        from this tensor. Specifically, it reads from memory locations\n        `in_ptr + 512` down to `in_ptr + 1`. For example, the value at\n        `in_ptr + 512` is read first (for `x0=0` in the original implementation)\n        and stored at `out_ptr + 0`.\n    out_ptr : tl.pointer_type\n        A pointer to the output tensor. The kernel writes the 512 reversed\n        elements to this tensor. Specifically, it writes to memory locations\n        `out_ptr + 0` through `out_ptr + 511`. For example, `out_ptr + 0`\n        receives the value from `in_ptr + 512`.\n    \"\"\"\n    # Your code here\n\n", "label": "######################################## Imports ######################################## \nimport numpy as np\nimport pytest\nimport torch\nfrom numpy.random import RandomState\n\nimport triton\nimport triton.language as tl\n######################################## Imports ######################################## \n\n@triton.jit\ndef reverse_range(in_ptr, out_ptr):\n    x0 = tl.arange(0, 512)\n    tmp0 = tl.load(in_ptr + (512 - x0))\n    tl.store(out_ptr + x0, tmp0)\n\n##################################################################################################################################################  \n\nimport numpy as np\nimport random\nimport torch \nimport os\nimport pytest\nfrom numpy.random import RandomState\nfrom tb_eval.perf.ROCm.performance_utils_pytest import PytestBenchmarker, do_bench_config, save_all_benchmark_results\nfrom typing import Dict\n\nresult_gold = {}\n\n######################################## HELPERS for Eval ######################################## \ndef set_seed(seed: int = 42) -> None:\n    \"\"\"\n    Set the random seed for reproducibility across multiple libraries and configure PyTorch for deterministic behavior.\n\n    Args:\n        seed (int): The seed value to set. Default is 42.\n    \"\"\"\n    # Set seed for Python's built-in random module\n    random.seed(seed)\n    # Set seed for NumPy\n    np.random.seed(seed)\n    # Set seed for PyTorch on CPU\n    torch.manual_seed(seed)\n    # Set seed for PyTorch on all GPUs (if available)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    # Ensure deterministic behavior in PyTorch\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set environment variable for hash-based operations\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n######################################## HELPERS for Eval ######################################## \n\n\n\ndef test_reverse_range(request, device='cuda'):\n    set_seed()\n    \n\n    data = torch.randn((516, ), dtype=torch.float32, device=device)\n    res = torch.empty((512, ), dtype=torch.float32, device=device)\n    reverse_range[(1, )](data, res)\n    ref = torch.flip(data[1:513], [0])\n\n    result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])\n    \n    ################### save tri_out in result_gold ###################\n    test_case_name = request.node.name\n    sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\")\n    result_gold[sanitized_key_name] = res.clone().detach().cpu()\n    ################################################################### \n\n\n    assert (res == ref).all()\n\n\n# --- Python wrapper for the kernel for benchmarking ---\ndef reverse_range_triton_wrapper(in_tensor_for_kernel, out_buffer, num_warps_launch):\n    # The kernel is hardcoded to process 512 elements.\n    # in_tensor_for_kernel should be the base pointer from which kernel reads data[1]..data[512]\n    # out_buffer is where results are written.\n    grid = (1,)\n    reverse_range[grid](\n        in_tensor_for_kernel, \n        out_buffer,\n        num_warps=num_warps_launch # Launch hint\n    )\n    return out_buffer\n\n# --- Define TFLOPS and GB/s calculators ---\nKERNEL_FIXED_SIZE = 512\n\ndef calculate_reverse_range_tflops(params: dict, ms: float) -> float:\n    # This is a memory copy operation, no significant arithmetic FLOPs.\n    return 0.0 \n\ndef calculate_reverse_range_gbps(params: dict, ms: float) -> float:\n    N_processed = KERNEL_FIXED_SIZE\n    dtype_str = params.get('dtype_str', 'fp32') \n\n    current_dtype = torch.float32\n    if dtype_str == 'fp16': current_dtype = torch.float16\n    elif dtype_str == 'bf16': current_dtype = torch.bfloat16\n    element_size = torch.tensor([], dtype=current_dtype).element_size()\n\n    # Reads N_processed elements, Writes N_processed elements\n    bytes_read = N_processed * element_size\n    bytes_write = N_processed * element_size \n    total_bytes = bytes_read + bytes_write\n    gbps = total_bytes / (ms / 1000) / 1e9\n    return gbps\n\nOP_NAME_FOR_BENCHMARK = \"reverse_range_triton_perf\"\n\n# --- Pytest parametrize for performance testing ---\n# Kernel size is fixed at 512. We can vary dtype and num_warps.\nREVERSE_RANGE_DTYPES_FOR_PERF = ['fp32', 'fp16', 'bf16'] \nREVERSE_RANGE_NUM_WARPS_FOR_PERF = [1, 2, 4] \n\n@pytest.mark.parametrize(\"dtype_str\", REVERSE_RANGE_DTYPES_FOR_PERF)\n@pytest.mark.parametrize(\"num_warps_launch\", REVERSE_RANGE_NUM_WARPS_FOR_PERF)\ndef test_performance(dtype_str, num_warps_launch, request, device='cuda'):\n    set_seed()\n    \n    if dtype_str == 'bf16':\n        cap = torch.cuda.get_device_capability()\n        if cap[0] < 8:\n            pytest.skip(\"bf16 requires Ampere+ (arch 80+)\")\n    \n    if dtype_str == 'fp32': current_dtype = torch.float32\n    elif dtype_str == 'bf16': current_dtype = torch.bfloat16\n    else: current_dtype = torch.float16\n\n    # Input tensor `data_perf` needs to be large enough for kernel's access pattern.\n    # Kernel reads from in_ptr[1] to in_ptr[512].\n    # So, `data_perf` needs at least 513 elements if passed directly.\n    # Original test used size 516 for `data`.\n    data_perf = torch.randn((KERNEL_FIXED_SIZE + 4, ), dtype=current_dtype, device=device) # e.g. 516 elements\n    # Output buffer `res_perf` is size 512.\n    res_perf_buffer = torch.empty((KERNEL_FIXED_SIZE, ), dtype=current_dtype, device=device)\n    \n    op_lambda = lambda: reverse_range_triton_wrapper(\n        data_perf, res_perf_buffer, num_warps_launch\n    )\n\n    bench_config = do_bench_config(warm_up=100, repetition=1000) # Simple kernel, more reps\n    benchmarker = PytestBenchmarker(op_callable=op_lambda,\n                                    op_name=OP_NAME_FOR_BENCHMARK,\n                                    config=bench_config)\n    current_params_for_logs_and_calc = {\n        \"N_processed\": KERNEL_FIXED_SIZE, \n        \"dtype_str\": dtype_str,\n        \"num_warps\": num_warps_launch\n    }\n    \n    perf_result = benchmarker.run_benchmark(current_params_dict=current_params_for_logs_and_calc,\n                                            gbps_calculator=calculate_reverse_range_gbps,\n                                            tflops_calculator=calculate_reverse_range_tflops)\n\n######################################## HELPERS for Eval ########################################     \n# --- Pytest hook to save the dictionary at the end of the session ---  \ndef test_save_results():  \n    \"\"\"  \n    Called after whole test run finished, right before returning the exit status to the system.  \n    \"\"\"\n    print('Inside session finish...')\n    if \"_CALL_SUCCESS_\" not in result_gold:\n        result_gold['_CALL_SUCCESS_'] = torch.tensor([[0.0]])\n    OUTPUT_FILENAME = __file__.replace('.','_') + '.pt'\n    print(f\"\\nSaving all y_triton results to {OUTPUT_FILENAME}...\")  \n    # Ensure the directory for the output file exists if it's in a subdirectory  \n    output_dir = os.path.dirname(OUTPUT_FILENAME)  \n    if output_dir and not os.path.exists(output_dir):  \n        os.makedirs(output_dir, exist_ok=True)  \n    torch.save(result_gold, OUTPUT_FILENAME)       \n    print(f\"Successfully saved {len(result_gold)} y_triton tensors to {OUTPUT_FILENAME}.\")  \n\n\ndef test_save_performance_results():\n    \"\"\"\n    Called after the test_performance function finishes.\n    This is a separate hook to ensure performance results are saved.\n    \"\"\"\n    print('\\nPytest session finishing... Saving benchmark results...')\n\n    output_directory = os.path.join(os.path.dirname(__file__), \"perf\")  # Save in a \"perf\" subdirectory next to the test file\n    os.makedirs(output_directory, exist_ok=True)\n    \n    save_all_benchmark_results(output_directory)\n    print(f\"All benchmark results attempted to save to: {output_directory}\")\n\n######################################## HELPERS for Eval ########################################"}, {"file": "test_block_pointer_matmul.py", "target_kernel_name": "matmul_no_scf_with_advance_kernel", "instruction": "\nYou are an expert in triton programming language. You will be given the function definition for the `matmul_no_scf_with_advance_kernel`. Your task is to complete the kernel code. Only complete the kernel code in the function definition, DONT remove any python imports or helper utils in the instruction/code provided, DONT change/interfere with the provided function definition and parameter list.\n\nThis kernel, `matmul_no_scf_with_advance_kernel`,  is designed to use block pointers for a basic matrix multiplication(without explicit loops for the K dimension, hence \"no_scf\" - no structured control flow)\n\n**Your objective is to implement the body of `matmul_no_scf_with_advance_kernel`.**\n\nYou must ensure that:\n1.  All arguments received by `matmul_no_scf_with_advance_kernel` are kept intact and not modified.\n2. Provide you final code in ```python code block. \nExample:\n```python\n<YOUR-CODE-HERE>\n```\nThe full definition for `matmul_no_scf_with_advance_kernel` and relevant helper utilities are provided in the context below. You only need to complete the code for `matmul_no_scf_with_advance_kernel` whilst keeping other things intact. DONT remove Imports and HELPER utils.\n\n######################################## Imports ######################################## \nimport pytest\nimport torch\n\nimport triton\nimport triton.language as tl\nimport os\n\n######################################## Imports ######################################## \n\n\n@triton.jit\ndef matmul_no_scf_with_advance_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    \"\"\"\n    Computes a block of the matrix multiplication C = A @ B.\n\n    This kernel is designed to calculate a single (BLOCK_M, BLOCK_N) tile of the output matrix C.\n    It loads a (BLOCK_M, BLOCK_K) tile from matrix A and a (BLOCK_K, BLOCK_N) tile from matrix B.\n    The kernel utilizes `tl.make_block_ptr` for creating pointers to blocks of A and B,\n    and demonstrates the use of `tl.advance` for adjusting these block pointers.\n    The core computation is performed using `tl.dot`. The resulting tile is then stored\n    back to the C matrix. This version does not use Triton's Structured Control Flow (SCF)\n    for iterating over the K dimension; it assumes BLOCK_K covers the necessary\n    portion of the K dimension for a single dot product accumulation or that accumulation\n    over K-blocks is handled externally.\n\n    Parameters:\n    -----------\n    a_ptr : tl.pointer_type\n        Pointer to the input matrix A.\n    b_ptr : tl.pointer_type\n        Pointer to the input matrix B.\n    c_ptr : tl.pointer_type\n        Pointer to the output matrix C.\n    M : int\n        The number of rows in matrix A and matrix C.\n    N : int\n        The number of columns in matrix B and matrix C.\n    K : int\n        The number of columns in matrix A and rows in matrix B.\n    stride_am : int\n        The stride (in elements) for moving from one row to the next in matrix A.\n    stride_ak : int\n        The stride (in elements) for moving from one column to the next in matrix A.\n    stride_bk : int\n        The stride (in elements) for moving from one row to the next in matrix B.\n    stride_bn : int\n        The stride (in elements) for moving from one column to the next in matrix B.\n    stride_cm : int\n        The stride (in elements) for moving from one row to the next in matrix C.\n    stride_cn : int\n        The stride (in elements) for moving from one column to the next in matrix C.\n    BLOCK_M : tl.constexpr\n        The height of the tiles processed from matrix A and C.\n    BLOCK_N : tl.constexpr\n        The width of the tiles processed from matrix B and C.\n    BLOCK_K : tl.constexpr\n        The depth of the tiles (common dimension K) processed from A and B for the dot product.\n    \"\"\"\n    # Your code here\n\n", "label": "######################################## Imports ######################################## \nimport pytest\nimport torch\n\nimport triton\nimport triton.language as tl\nimport os\n######################################## Imports ######################################## \n\n@triton.jit\ndef matmul_no_scf_with_advance_kernel(  #\n        a_ptr, b_ptr, c_ptr,  #\n        M, N, K,  #\n        stride_am, stride_ak,  #\n        stride_bk, stride_bn,  #\n        stride_cm, stride_cn,  #\n        BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr  #\n):\n    offs_m = tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    a_block_ptr = tl.make_block_ptr(base=a_ptr, shape=(M, K), strides=(stride_am, stride_ak), offsets=(0, 0),\n                                    block_shape=(BLOCK_M, BLOCK_K), order=(1, 0))\n    b_block_ptr = tl.make_block_ptr(base=b_ptr, shape=(K, N), strides=(stride_bk, stride_bn), offsets=(0, 0),\n                                    block_shape=(BLOCK_K, BLOCK_N), order=(1, 0))\n    # Below two lines are just for testing negative offsets for the `advance` API, which could be removed\n    a_block_ptr = tl.advance(a_block_ptr, (BLOCK_M, -BLOCK_K))\n    a_block_ptr = tl.advance(a_block_ptr, (-BLOCK_M, BLOCK_K))\n    a = tl.load(a_block_ptr, boundary_check=(1, ), padding_option=\"zero\")\n    b = tl.load(b_block_ptr, boundary_check=(0, ), padding_option=\"zero\")\n\n    c = tl.dot(a, b)\n    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n    tl.store(c_ptrs, c)\n\n##################################################################################################################################################  \n\nimport numpy as np\nimport random\nimport torch \nimport os\nimport pytest\nfrom numpy.random import RandomState\nfrom tb_eval.perf.ROCm.performance_utils_pytest import PytestBenchmarker, do_bench_config, save_all_benchmark_results\nfrom typing import Dict\n\nresult_gold = {}\n\n######################################## HELPERS for Eval ######################################## \ndef set_seed(seed: int = 42) -> None:\n    \"\"\"\n    Set the random seed for reproducibility across multiple libraries and configure PyTorch for deterministic behavior.\n\n    Args:\n        seed (int): The seed value to set. Default is 42.\n    \"\"\"\n    # Set seed for Python's built-in random module\n    random.seed(seed)\n    # Set seed for NumPy\n    np.random.seed(seed)\n    # Set seed for PyTorch on CPU\n    torch.manual_seed(seed)\n    # Set seed for PyTorch on all GPUs (if available)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    # Ensure deterministic behavior in PyTorch\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set environment variable for hash-based operations\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n######################################## HELPERS for Eval ######################################## \n\n\n@pytest.mark.interpreter\n@pytest.mark.parametrize(\"shape, num_warps\", [  #\n    (shape, num_warps) for shape in [\n        [64, 64, 16],\n        [64, 64, 32],\n        [64, 64, 64],\n    ] for num_warps in [4, 8]\n])\ndef test_block_ptr_matmul_no_scf(shape, num_warps, request, device='cuda'):\n    set_seed()\n\n    m, n, k = shape\n    a = torch.randn((m, k), device=device, dtype=torch.float16)\n    b = torch.randn((k, n), device=device, dtype=torch.float16)\n    c = torch.empty((m, n), device=device, dtype=torch.float32)\n\n    grid = lambda META: (1, )\n    matmul_no_scf_with_advance_kernel[grid](\n        a_ptr=a, b_ptr=b, c_ptr=c,  #\n        M=m, N=n, K=k,  #\n        stride_am=a.stride(0), stride_ak=a.stride(1),  #\n        stride_bk=b.stride(0), stride_bn=b.stride(1),  #\n        stride_cm=c.stride(0), stride_cn=c.stride(1),  #\n        BLOCK_M=m, BLOCK_N=n, BLOCK_K=k,  #\n        num_warps=num_warps)\n    golden = torch.matmul(a, b)\n\n    ################### save tri_out in result_gold ###################\n    test_case_name = request.node.name\n    sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\")\n    result_gold[sanitized_key_name] = c.clone().detach().cpu()\n    ################################################################### \n\n    result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])\n    \n    torch.testing.assert_close(c, golden, check_dtype=False)\n\n# --- Python wrapper for the kernel ---\ndef block_pointer_matmul_triton_wrapper(a_tensor, b_tensor, c_tensor, num_warps_arg):\n    m, k_a = a_tensor.shape\n    k_b, n = b_tensor.shape\n    assert k_a == k_b, \"K dimension must match\"\n    \n    # The kernel is designed for a single block operation covering the whole matrix\n    # So, BLOCK_M=m, BLOCK_N=n, BLOCK_K=k_a\n    # Grid is (1,) as per original test\n    grid = (1,)\n    \n    matmul_no_scf_with_advance_kernel[grid](\n        a_ptr=a_tensor, b_ptr=b_tensor, c_ptr=c_tensor,\n        M=m, N=n, K=k_a,\n        stride_am=a_tensor.stride(0), stride_ak=a_tensor.stride(1),\n        stride_bk=b_tensor.stride(0), stride_bn=b_tensor.stride(1),\n        stride_cm=c_tensor.stride(0), stride_cn=c_tensor.stride(1),\n        BLOCK_M=m, BLOCK_N=n, BLOCK_K=k_a, # Kernel uses these as constexpr\n        num_warps=num_warps_arg # num_warps passed to launch, not directly used by this jit kernel signature\n                                # but can affect autotuning if kernel had it. This kernel is not autotuned.\n    )\n    return c_tensor\n\n# --- Define TFLOPS and GB/s calculators for this specific GEMM ---\ndef calculate_block_matmul_tflops(params: dict, ms: float) -> float:\n    M, N, K = params['M'], params['N'], params['K']\n    # Single dot product of (M,K) @ (K,N)\n    flops = 2 * M * N * K\n    tflops = flops / (ms / 1000) / 1e12\n    return tflops\n\ndef calculate_block_matmul_gbps(params: dict, ms: float) -> float:\n    M, N, K = params['M'], params['N'], params['K']\n    dtype_str = params.get('dtype_str', 'fp16') # Original test uses fp16\n\n    if dtype_str == 'fp32': current_dtype = torch.float32\n    elif dtype_str == 'bf16': current_dtype = torch.bfloat16\n    else: current_dtype = torch.float16\n    element_size = torch.tensor([], dtype=current_dtype).element_size()\n\n    bytes_a = M * K * element_size\n    bytes_b = K * N * element_size\n    bytes_c = M * N * element_size\n    total_bytes = bytes_a + bytes_b + bytes_c\n    gbps = total_bytes / (ms / 1000) / 1e9\n    return gbps\n\n# --- Name for the benchmark output JSON file ---\nOP_NAME_FOR_BENCHMARK = \"block_pointer_matmul_perf\"\n\n# --- Pytest parametrize for performance testing ---\n# Original shapes are small. For perf, we might want larger, but respecting kernel's nature.\n# This kernel does M*K, K*N, M*N as ONE block.\n# So, M, N, K are also BLOCK_M, BLOCK_N, BLOCK_K.\n# Triton has limits on BLOCK_SIZEs (e.g., BLOCK_K often <= 1024 or 2048 due to shared memory).\n# Let's use shapes where M, N, K themselves are valid block sizes.\nBPM_SHAPES_FOR_PERF = [\n    # M,  N,   K\n    (64, 64,  64),\n    (128, 128, 128),\n    (64, 128, 256),\n    (256, 64,  128),\n    (128, 256, 64),\n]\nBPM_DTYPES_FOR_PERF = ['fp16', 'fp32'] # Add bf16 if relevant\n# num_warps is passed to launch but not a JIT param for this kernel.\n# It can influence scheduling. Let's fix it for perf or parametrize.\nBPM_NUM_WARPS_FOR_PERF = [4, 8]\n\n\n@pytest.mark.parametrize(\"shape\", BPM_SHAPES_FOR_PERF)\n@pytest.mark.parametrize(\"num_warps_arg\", BPM_NUM_WARPS_FOR_PERF)\n@pytest.mark.parametrize(\"dtype_str\", BPM_DTYPES_FOR_PERF)\ndef test_performance(shape, num_warps_arg, dtype_str, request, device='cuda'): # Renamed\n    set_seed()\n    m, n, k = shape\n\n    if dtype_str == 'fp32': current_dtype = torch.float32\n    elif dtype_str == 'bf16': current_dtype = torch.bfloat16\n    else: current_dtype = torch.float16\n\n    a = torch.randn((m, k), device=device, dtype=current_dtype)\n    b = torch.randn((k, n), device=device, dtype=current_dtype)\n    # Output c for this kernel is float32 if inputs are float16, due to tl.dot default accumulator.\n    # The kernel does `c_val.to(c_ptr.dtype.element_ty)`. So c's dtype matters.\n    # Let's make c's dtype match input for simplicity, or be float32 if inputs are lower precision.\n    # If a,b are fp16, c = torch.dot(a,b) accumulates in fp32, then cast to c.dtype.\n    # For perf, often C is same dtype as A, B or higher precision (fp32).\n    # Let C be same dtype as input for this benchmark.\n    c = torch.empty((m, n), device=device, dtype=current_dtype)\n\n\n    # --- Create op_lambda for benchmarking ---\n    op_lambda = lambda: block_pointer_matmul_triton_wrapper(a, b, c, num_warps_arg)\n\n    # --- Benchmarking ---\n    bench_config = do_bench_config(warm_up=25, repetition=100)\n    benchmarker = PytestBenchmarker(op_callable=op_lambda,\n                                    op_name=OP_NAME_FOR_BENCHMARK,\n                                    config=bench_config)\n\n    current_params_for_logs_and_calc = {\n        \"M\": m, \"N\": n, \"K\": k,\n        \"num_warps\": num_warps_arg, \"dtype_str\": dtype_str\n    }\n\n    benchmarker.run_benchmark(current_params_dict=current_params_for_logs_and_calc,\n                              gbps_calculator=calculate_block_matmul_gbps,\n                              tflops_calculator=calculate_block_matmul_tflops)\n\n######################################## HELPERS for Eval ########################################     \n# --- Pytest hook to save the dictionary at the end of the session ---  \ndef test_save_results():  \n    \"\"\"  \n    Called after whole test run finished, right before returning the exit status to the system.  \n    \"\"\"\n    print('Inside session finish...')\n    if \"_CALL_SUCCESS_\" not in result_gold:\n        result_gold['_CALL_SUCCESS_'] = torch.tensor([[0.0]])\n    OUTPUT_FILENAME = __file__.replace('.','_') + '.pt'\n    print(f\"\\nSaving all y_triton results to {OUTPUT_FILENAME}...\")  \n    # Ensure the directory for the output file exists if it's in a subdirectory  \n    output_dir = os.path.dirname(OUTPUT_FILENAME)  \n    if output_dir and not os.path.exists(output_dir):  \n        os.makedirs(output_dir, exist_ok=True)  \n    torch.save(result_gold, OUTPUT_FILENAME)       \n    print(f\"Successfully saved {len(result_gold)} y_triton tensors to {OUTPUT_FILENAME}.\")  \n\ndef test_save_performance_results():\n    \"\"\"\n    Called after the test_performance function finishes.\n    This is a separate hook to ensure performance results are saved.\n    \"\"\"\n    print('\\nPytest session finishing... Saving benchmark results...')\n\n    output_directory = os.path.join(os.path.dirname(__file__), \"perf\")  # Save in a \"perf\" subdirectory next to the test file\n    os.makedirs(output_directory, exist_ok=True)\n    \n    save_all_benchmark_results(output_directory)\n    print(f\"All benchmark results attempted to save to: {output_directory}\")\n\n######################################## HELPERS for Eval ########################################"}, {"file": "test_tma_store_gemm.py", "target_kernel_name": "matmul_tma_load_store", "instruction": "\nYou are an expert in triton programming language. You will be given the function definition for the `matmul_tma_load_store`. Your task is to complete the kernel code. Only complete the kernel code in the function definition, DONT remove any python imports or helper utils in the instruction/code provided, DONT change/interfere with the provided function definition and parameter list.\n\nThis kernel, `matmul_tma_load_store`,  performs a single block matrix multiplication (C = A @ B) using Triton's block pointers using TMA (Tensor Memory Accelerator).\n\n**Your objective is to implement the body of `matmul_tma_load_store`.**\n\nYou must ensure that:\n1.  All arguments received by `matmul_tma_load_store` are kept intact and not modified.\n2. Provide you final code in ```python code block. \nExample:\n```python\n<YOUR-CODE-HERE>\n```\nThe full definition for `matmul_tma_load_store` and relevant helper utilities are provided in the context below. You only need to complete the code for `matmul_tma_load_store` whilst keeping other things intact. DONT remove Imports and HELPER utils.\n\n######################################## Imports ########################################\n\nimport pytest\nimport torch\nfrom torch.testing import assert_close\n\nimport triton\nimport triton.language as tl\n######################################## Imports ########################################\n\n\n@triton.jit\ndef matmul_tma_load_store(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n    OUTPUT_F16: tl.constexpr\n):\n    \"\"\"\n    Performs a single block matrix multiplication (C = A @ B) using Triton's\n    block pointers, potentially leveraging TMA (Tensor Memory Accelerator)\n    for efficient loads and stores on compatible hardware.\n\n    This kernel is designed to compute one `BLOCK_M x BLOCK_N` tile of the output matrix C.\n    Specifically, it loads a `BLOCK_M x BLOCK_K` tile from matrix A (starting from `a_ptr`\n    at offset (0,0)) and a `BLOCK_K x BLOCK_N` tile from matrix B (starting from `b_ptr`\n    at offset (0,0)). It then computes their dot product and stores the resulting\n    `BLOCK_M x BLOCK_N` tile into matrix C (starting at `c_ptr` at offset (0,0)).\n\n    The kernel uses `tl.load` and `tl.store` with block pointers configured as follows:\n    - Matrix A's tile is loaded assuming a row-major layout within the block (`order=(1,0)`).\n    - Matrix B's tile is loaded assuming a column-major layout within the block (`order=(0,1)`),\n      which is often beneficial for dot product operations.\n    - Matrix C's tile is stored assuming a row-major layout within the block (`order=(1,0)`).\n\n    Input matrices A and B are expected to have data types suitable for `tl.dot`\n    (e.g., tl.float16, tl.bfloat16, tl.float32). The accumulation for the dot\n    product is typically performed in tl.float32.\n\n    Args:\n        a_ptr: Pointer to the base of the input matrix A in global memory.\n        b_ptr: Pointer to the base of the input matrix B in global memory.\n        c_ptr: Pointer to the base of the output matrix C in global memory.\n        M (int): The total number of rows in the full matrix A and matrix C. Used for boundary checks.\n        N (int): The total number of columns in the full matrix B and matrix C. Used for boundary checks.\n        K (int): The total number of columns in the full matrix A and rows in matrix B\n                 (the common dimension for matrix multiplication). Used for boundary checks.\n        stride_am (int): Stride in number of elements to move from one row to the next in matrix A.\n        stride_ak (int): Stride in number of elements to move from one column to the next in matrix A.\n        stride_bk (int): Stride in number of elements to move from one row to the next in matrix B.\n        stride_bn (int): Stride in number of elements to move from one column to the next in matrix B.\n        stride_cm (int): Stride in number of elements to move from one row to the next in matrix C.\n        stride_cn (int): Stride in number of elements to move from one column to the next in matrix C.\n        BLOCK_M (tl.constexpr): The height (number of rows) of the tile to be processed from matrix A\n                                and written to matrix C. This defines the M-dimension of the block.\n        BLOCK_N (tl.constexpr): The width (number of columns) of the tile to be processed from matrix B\n                                and written to matrix C. This defines the N-dimension of the block.\n        BLOCK_K (tl.constexpr): The width (number of columns) of the tile from matrix A, and height\n                                (number of rows) of the tile from matrix B. This defines the\n                                K-dimension of the blocks used in the dot product.\n        OUTPUT_F16 (tl.constexpr): A boolean flag. If True, the resulting C tile is cast to\n                                   `tl.float16` before being stored. Otherwise, it is stored\n                                   in the accumulation data type (typically `tl.float32`).\n    \"\"\"\n    # Your code here\n\n", "label": "\n\n# Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n#\n# Permission is hereby granted, free of charge, to any person obtaining\n# a copy of this software and associated documentation files\n# (the \"Software\"), to deal in the Software without restriction,\n# including without limitation the rights to use, copy, modify, merge,\n# publish, distribute, sublicense, and/or sell copies of the Software,\n# and to permit persons to whom the Software is furnished to do so,\n# subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be\n# included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n# MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n# IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n# CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n# SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n######################################## Imports ######################################## \n\nimport pytest\nimport torch\nfrom torch.testing import assert_close\n\nimport triton\nimport triton.language as tl\n######################################## Imports ######################################## \n\n\n@triton.jit\ndef matmul_tma_load_store(  #\n        a_ptr, b_ptr, c_ptr,  #\n        M, N, K,  #\n        stride_am, stride_ak,  #\n        stride_bk, stride_bn,  #\n        stride_cm, stride_cn,  #\n        BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,  #\n        OUTPUT_F16: tl.constexpr  #\n):\n    a_block_ptr = tl.make_block_ptr(base=a_ptr, shape=(M, K), strides=(stride_am, stride_ak), offsets=(0, 0),\n                                    block_shape=(BLOCK_M, BLOCK_K), order=(1, 0))\n    b_block_ptr = tl.make_block_ptr(base=b_ptr, shape=(K, N), strides=(stride_bk, stride_bn), offsets=(0, 0),\n                                    block_shape=(BLOCK_K, BLOCK_N), order=(0, 1))\n    c_block_ptr = tl.make_block_ptr(base=c_ptr, shape=(M, N), strides=(stride_cm, stride_cn), offsets=(0, 0),\n                                    block_shape=(BLOCK_M, BLOCK_N), order=(1, 0))\n    a = tl.load(a_block_ptr)\n    b = tl.load(b_block_ptr)\n\n    c = tl.dot(a, b)\n    if OUTPUT_F16:\n        c = c.to(tl.float16)\n\n    tl.store(c_block_ptr, c)\n\n##################################################################################################################################################  \n\n\nimport pytest\nimport numpy as np\nimport random\nimport torch \nimport os\nimport pytest\nfrom torch.testing import assert_close\nfrom tb_eval.perf.ROCm.performance_utils_pytest import PytestBenchmarker, do_bench_config, save_all_benchmark_results\nfrom typing import Dict\n\n\nresult_gold = {}\n\n######################################## HELPERS for Eval ######################################## \ndef set_seed(seed: int = 42) -> None:\n    \"\"\"\n    Set the random seed for reproducibility across multiple libraries and configure PyTorch for deterministic behavior.\n\n    Args:\n        seed (int): The seed value to set. Default is 42.\n    \"\"\"\n    # Set seed for Python's built-in random module\n    random.seed(seed)\n    # Set seed for NumPy\n    np.random.seed(seed)\n    # Set seed for PyTorch on CPU\n    torch.manual_seed(seed)\n    # Set seed for PyTorch on all GPUs (if available)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    # Ensure deterministic behavior in PyTorch\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set environment variable for hash-based operations\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n######################################## HELPERS for Eval ######################################## \n\n\n\n@pytest.mark.parametrize('M,N,K,NUM_CTAS,NUM_WARPS,TRANS_A,TRANS_B,OUTPUT_F16', [\n    [64, 64, 16, 1, 4, False, True, False],\n    [64, 64, 16, 1, 4, False, True, True],\n    [128, 64, 32, 1, 4, False, True, False],\n    [128, 64, 32, 1, 4, False, True, True],\n    [64, 128, 32, 1, 4, False, True, False],\n    [64, 128, 32, 1, 4, False, True, True],\n    [128, 128, 64, 1, 4, False, True, False],\n    [128, 128, 64, 1, 4, False, True, True],\n])\ndef test_tma_load_store(M, N, K, NUM_CTAS, NUM_WARPS, TRANS_A, TRANS_B, OUTPUT_F16, request):\n    set_seed()\n\n    if (TRANS_A):\n        a = torch.randn((K, M), device='cuda', dtype=torch.float16).T\n    else:\n        a = torch.randn((M, K), device='cuda', dtype=torch.float16)\n    if (TRANS_B):\n        b = torch.randn((N, K), device='cuda', dtype=torch.float16).T\n    else:\n        b = torch.randn((K, N), device='cuda', dtype=torch.float16)\n\n    c = torch.empty((M, N), device=a.device, dtype=torch.float32)\n    if OUTPUT_F16:\n        c = torch.empty((M, N), device=a.device, dtype=torch.float16)\n\n    matmul_tma_load_store[(1, 1)](\n        a_ptr=a, b_ptr=b, c_ptr=c,  #\n        M=M, N=N, K=K,  #\n        stride_am=a.stride(0), stride_ak=a.stride(1),  #\n        stride_bk=b.stride(0), stride_bn=b.stride(1),  #\n        stride_cm=c.stride(0), stride_cn=c.stride(1),  #\n        BLOCK_M=M, BLOCK_N=N, BLOCK_K=K,  #\n        num_warps=NUM_WARPS, num_ctas=NUM_CTAS,  #\n        OUTPUT_F16=OUTPUT_F16)\n    golden = torch.matmul(a, b)\n    torch.set_printoptions(profile=\"full\")\n\n    result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])\n    \n    ################### save tri_out in result_gold ###################\n    test_case_name = request.node.name\n    sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\")\n    result_gold[sanitized_key_name] = c.clone().detach().cpu()\n    ###################################################################\n\n    assert_close(c, golden, rtol=1e-2, atol=1e-3, check_dtype=False)\n\n\n# --- Python wrapper for the kernel for benchmarking ---\ndef tma_gemm_triton_wrapper(a_tensor, b_tensor, c_buffer, \n                            M_dim, N_dim, K_dim, \n                            output_f16_flag, num_warps_launch): # num_ctas_launch removed as not used by grid\n    grid = (1,) \n    matmul_tma_load_store[grid](\n        a_ptr=a_tensor, b_ptr=b_tensor, c_ptr=c_buffer,\n        M=M_dim, N=N_dim, K=K_dim,\n        stride_am=a_tensor.stride(0), stride_ak=a_tensor.stride(1),\n        stride_bk=b_tensor.stride(0), stride_bn=b_tensor.stride(1),\n        stride_cm=c_buffer.stride(0), stride_cn=c_buffer.stride(1),\n        BLOCK_M=M_dim, BLOCK_N=N_dim, BLOCK_K=K_dim, \n        OUTPUT_F16=output_f16_flag,\n        num_warps=num_warps_launch\n        # num_ctas=num_ctas_launch # Launch hint, not JIT param\n    )\n    return c_buffer\n\ndef calculate_tma_gemm_tflops(params: dict, ms: float) -> float:\n    M, N, K = params['M'], params['N'], params['K']\n    flops = 2 * M * N * K \n    tflops = flops / (ms / 1000) / 1e12\n    return tflops\n\n    \ndef calculate_tma_gemm_gbps(params: dict, ms: float) -> float:\n    M, N, K = params['M'], params['N'], params['K']\n    \n    # Input dtype is fp16 as per test setup\n    input_dtype_for_calc = torch.float16\n    # Output C can be fp16 or fp32\n    output_dtype_str = \"fp16\" if params['OUTPUT_F16'] else \"fp32\"\n    \n    output_torch_dtype_for_calc = torch.float16\n    if output_dtype_str == 'fp32': output_torch_dtype_for_calc = torch.float32\n    # No bf16 in this test's parametrize for output\n\n    in_element_size = torch.tensor([], dtype=input_dtype_for_calc).element_size()\n    out_element_size = torch.tensor([], dtype=output_torch_dtype_for_calc).element_size()\n    \n    bytes_a = M * K * in_element_size\n    bytes_b = K * N * in_element_size \n    bytes_c_write = M * N * out_element_size\n    total_bytes = bytes_a + bytes_b + bytes_c_write\n    gbps = total_bytes / (ms / 1000) / 1e9\n    return gbps\n\nOP_NAME_FOR_BENCHMARK = \"tma_store_gemm_triton_perf\"\n\nTMA_GEMM_PERF_PARAMS = [ \n    [64, 64, 16, 1, 4, False, True, False], [64, 64, 16, 1, 4, False, True, True],\n    [128, 64, 32, 1, 4, False, True, False], [128, 64, 32, 1, 4, False, True, True],\n    [64, 128, 32, 1, 4, False, True, False], [64, 128, 32, 1, 4, False, True, True],\n    [128, 128, 64, 1, 4, False, True, False], [128, 128, 64, 1, 4, False, True, True],\n    [64, 64, 128, 1, 4, False, False, True], \n    [64, 64, 128, 1, 4, False, False, False],\n    [32, 32, 256, 1, 4, False, False, True],\n]\n\n@pytest.mark.parametrize('M,N,K,NUM_CTAS_param,NUM_WARPS_param,TRANS_A,TRANS_B,OUTPUT_F16_param', TMA_GEMM_PERF_PARAMS)\n# @pytest.mark.skipif(torch.cuda.get_device_capability()[0] < 9, reason=\"Requires compute capability >= 9\")\ndef test_performance(M, N, K, NUM_CTAS_param, NUM_WARPS_param, TRANS_A, TRANS_B, OUTPUT_F16_param, request):\n    cap = torch.cuda.get_device_capability()\n    if cap[0] < 9: \n        pytest.skip(\"Requires compute capability >= 9\")\n\n    # --- CORRECTED Shared memory skip logic ---\n    input_torch_dtype_for_smem_check = torch.float16 # Inputs A and B are fp16 in this test\n    element_size_bytes_for_smem_check = torch.tensor([], dtype=input_torch_dtype_for_smem_check).element_size()\n    \n    smem_elements_needed = (M * K) + (K * N) # For one dot A(M,K) @ B(K,N)\n    if smem_elements_needed * element_size_bytes_for_smem_check > 65536: # 64KB limit\n        pytest.skip(f\"Skipping M{M}N{N}K{K} due to estimated shared memory for inputs: \"\n                    f\"{smem_elements_needed * element_size_bytes_for_smem_check} > 65536\")\n    # --- End of corrected skip logic ---\n\n    set_seed()\n        \n    # Input setup from original test, input dtype is fp16\n    input_torch_dtype = torch.float16\n    a_orig_shape = (K, M) if TRANS_A else (M, K)\n    b_orig_shape = (N, K) if TRANS_B else (K, N) # If TRANS_B, B is (N,K), so B.T is (K,N) for matmul\n    \n    a = torch.randn(a_orig_shape, device='cuda', dtype=input_torch_dtype)\n    if TRANS_A: a = a.T \n    \n    b = torch.randn(b_orig_shape, device='cuda', dtype=input_torch_dtype)\n    if TRANS_B: b = b.T \n\n    c_out_torch_dtype = torch.float16 if OUTPUT_F16_param else torch.float32\n    c_buffer = torch.empty((M, N), device=a.device, dtype=c_out_torch_dtype)\n\n    op_lambda = lambda: tma_gemm_triton_wrapper(\n        a, b, c_buffer, M, N, K, \n        OUTPUT_F16_param, NUM_WARPS_param\n    )\n\n    bench_config = do_bench_config(warm_up=25, repetition=100)\n    benchmarker = PytestBenchmarker(op_callable=op_lambda,\n                                    op_name=OP_NAME_FOR_BENCHMARK,\n                                    config=bench_config)\n    current_params_for_logs_and_calc = {\n        \"M\": M, \"N\": N, \"K\": K, \n        \"NUM_CTAS\": NUM_CTAS_param, \"NUM_WARPS\": NUM_WARPS_param,\n        \"TRANS_A\": TRANS_A, \"TRANS_B\": TRANS_B, \n        \"OUTPUT_F16\": OUTPUT_F16_param, # This is for the calculator\n        \"input_dtype_str\": \"fp16\" # For the calculator\n    }\n    \n    perf_result = benchmarker.run_benchmark(current_params_dict=current_params_for_logs_and_calc,\n                                            gbps_calculator=calculate_tma_gemm_gbps,\n                                            tflops_calculator=calculate_tma_gemm_tflops)\n\n######################################## HELPERS for Eval ########################################     \n# --- Pytest hook to save the dictionary at the end of the session ---  \ndef test_save_results():  \n    \"\"\"  \n    Called after whole test run finished, right before returning the exit status to the system.  \n    \"\"\"\n    print('Inside session finish...')\n    if \"_CALL_SUCCESS_\" not in result_gold:\n        result_gold['_CALL_SUCCESS_'] = torch.tensor([[0.0]])\n    OUTPUT_FILENAME = __file__.replace('.','_') + '.pt'\n    print(f\"\\nSaving all y_triton results to {OUTPUT_FILENAME}...\")  \n    # Ensure the directory for the output file exists if it's in a subdirectory  \n    output_dir = os.path.dirname(OUTPUT_FILENAME)  \n    if output_dir and not os.path.exists(output_dir):  \n        os.makedirs(output_dir, exist_ok=True)  \n    torch.save(result_gold, OUTPUT_FILENAME)       \n    print(f\"Successfully saved {len(result_gold)} y_triton tensors to {OUTPUT_FILENAME}.\")  \n\n\ndef test_save_performance_results():\n    \"\"\"\n    Called after the test_performance function finishes.\n    This is a separate hook to ensure performance results are saved.\n    \"\"\"\n    print('\\nPytest session finishing... Saving benchmark results...')\n\n    output_directory = os.path.join(os.path.dirname(__file__), \"perf\")  # Save in a \"perf\" subdirectory next to the test file\n    os.makedirs(output_directory, exist_ok=True)\n    \n    save_all_benchmark_results(output_directory)\n    print(f\"All benchmark results attempted to save to: {output_directory}\")\n\n######################################## HELPERS for Eval ########################################"}, {"file": "naive_softmax.py", "target_kernel_name": "softmax_kernel_naive", "instruction": "\nYou are an expert in triton programming language. You will be given the function definition for the `softmax_kernel_naive` kernel. Your task is to complete the kernel code. Only complete the kernel code in the function definition, DONT remove any python imports or helper utils in the instruction/code provided, DONT change/interfere with the provided function definition and parameter list.\n\nThis kernel, `softmax_kernel_naive`,  naive softmax operation.\n\n**Your objective is to implement the body of  the kernel `softmax_kernel_naive`.**\n\nYou must ensure that:\n1.  All arguments received by `softmax_kernel_naive` are kept intact and not modified.\n2. Provide your final code in ```python code block. \nExample:\n```python\n<YOUR-CODE-HERE>\n```\nThe full definitions for `softmax_kernel_naive` and relevant helper utilities are provided in the context below. You only need to complete the code for `softmax_kernel_naive` whilst keeping other things intact. DONT remove Imports and HELPER utils.\n\n######################################## Imports ########################################\nimport pytest\nimport torch\nfrom torch.testing import assert_close\n\nimport triton\nimport triton.language as tl\n\ndtype_mapping = {\n    'float16': torch.float16,\n    'float32': torch.float32,\n}\n######################################## Imports ########################################\n\n\n @triton.jit\ndef softmax_kernel_naive(in_ptr, output_ptr, row_stride, n_cols, BLOCK_SIZE: tl.constexpr):\n    \"\"\"\n    Computes the softmax function over the last dimension of a 2D input tensor.\n\n    Each program instance is responsible for processing a single row of the input tensor.\n\n    Parameters\n    ----------\n    in_ptr\n        Pointer to the 2D input tensor.\n    output_ptr\n        Pointer to the 2D output tensor where the result is stored. The dimensions\n        of this tensor should match the input tensor.\n    row_stride\n        The number of elements to skip in memory to move from the start of one\n        row to the start of the next. This is used to correctly index into the\n        input and output tensors.\n    n_cols\n        The size of the last dimension of the tensor (i.e., the number of columns\n        in each row).\n    BLOCK_SIZE : tl.constexpr\n        A compile-time constant that defines the size of the data block that each\n        instance processes in a single operation. This is used to tile the\n        computation over the columns of a row.\n    \"\"\"    # Each program instance processes a single row of the input tensor.\n    # 1. Get the row index\n    row_idx = tl.program_id(axis=0)\n\n    # 2. Compute offsets for the current row.\n    # The naive kernel assumes that the number of columns is a power of 2.\n    # and that `BLOCK_SIZE` is equal to `n_cols`.\n    row_start_ptr = in_ptr + row_idx * row_stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    input_ptrs = row_start_ptr + col_offsets\n\n    # 3. Load the row into a 1D block.\n    # `mask` is used to handle rows where `n_cols` is not a power of 2.\n    mask = col_offsets < n_cols\n    # load the input data; use `other=-float('inf')` to ensure correct `max` calculation\n    row = tl.load(input_ptrs, mask=mask, other=-float('inf'))\n\n    # 4. Compute softmax.\n    #    a. Subtract the maximum value for numerical stability.\n    row_minus_max = row - tl.max(row, axis=0)\n    #    b. Compute the numerator.\n    numerator = tl.exp(row_minus_max)\n    #    c. Compute the denominator.\n    denominator = tl.sum(numerator, axis=0)\n    #    d. Normalize.\n    softmax_output = numerator / denominator\n\n    # 5. Write the result to the output tensor.\n    output_row_start_ptr = output_ptr + row_idx * row_stride\n    output_ptrs = output_row_start_ptr + col_offsets\n    tl.store(output_ptrs, softmax_output, mask=mask)\n\n", "label": "# Copyright(C) [2025] Advanced Micro Devices, Inc. All rights reserved.\n#Imports \nimport argparse\nimport torch\nimport sys\nimport pytest\n\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef softmax_kernel_naive(in_ptr, output_ptr, row_stride, n_cols, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(0)\n\n    in_max = -float('inf')\n    for offset in range(0, n_cols, BLOCK_SIZE):\n        col_range = tl.arange(0, BLOCK_SIZE)\n        col_mask = col_range + offset < n_cols\n        in_data = tl.load(in_ptr + pid * row_stride + col_range + offset, mask=col_mask, other=-float('inf'))\n        in_max = tl.maximum(in_max, tl.max(in_data, axis=-1))\n    \n    in_exp_sum = 0.0\n    for offset in range(0, n_cols, BLOCK_SIZE):\n        col_range = tl.arange(0, BLOCK_SIZE)\n        col_mask = col_range + offset < n_cols\n        in_data = tl.load(in_ptr + pid * row_stride + col_range + offset, mask=col_mask, other=-float('inf'))\n        in_exp_sum = in_exp_sum + tl.sum(tl.exp(in_data - in_max), axis=-1)\n    \n    for offset in range(0, n_cols, BLOCK_SIZE):\n        col_range = tl.arange(0, BLOCK_SIZE)\n        col_mask = col_range + offset < n_cols\n        in_data = tl.load(in_ptr + pid * row_stride + col_range + offset, mask=col_mask)\n        in_exp = tl.exp(in_data - in_max)\n        tl.store(output_ptr + pid * row_stride + col_range + offset, in_exp / in_exp_sum, mask=col_mask)\n\n\n\n##################################################################################################################################################  \n\n\nimport numpy as np\nimport random\nimport torch \nimport os\nfrom tb_eval.perf.ROCm.performance_utils_pytest import PytestBenchmarker, do_bench_config, save_all_benchmark_results\nfrom typing import Dict\n\n\nresult_gold = {}\n\n######################################## HELPERS for Eval ######################################## \nCONFIG = {\n  \"llama3\": {\n    \"8B\": {\n      \"num_attention_heads\": 32,\n      \"num_key_value_heads\": 8,\n      \"hidden_size\": 4096,\n      \"intermediate_size\": 14336,\n      \"vocab_size\": 128256\n    },\n    \"70B\": {\n      \"num_attention_heads\": 64,\n      \"num_key_value_heads\": 8,\n      \"hidden_size\": 8192,\n      \"intermediate_size\": 28672,\n      \"vocab_size\": 128256\n    },\n    \"405B\": {\n      \"num_attention_heads\": 128,\n      \"num_key_value_heads\": 8,\n      \"hidden_size\": 16384,\n      \"intermediate_size\": 53248,\n      \"vocab_size\": 128256\n    }\n  },\n  \"mistral\": {\n    \"7B\": {\n      \"hidden_size\": 4096,\n      \"intermediate_size\": 14336,\n      \"num_attention_heads\": 32,\n      \"num_key_value_heads\": 8,\n      \"vocab_size\": 32000\n    },\n    \"22B\": {\n      \"hidden_size\": 6144,\n      \"intermediate_size\": 16384,\n      \"num_attention_heads\": 48,\n      \"num_key_value_heads\": 8,\n      \"vocab_size\": 32000\n    }\n\n  }\n}\n\n  \ndef get_model_configs(config_path='model_configs.json', model_families=[\"llama3\"], model=\"all\"):  \n    \"\"\"  \n    Load model names from the configuration file.  \n  \n    Args:  \n        config_path (str): User-provided path to the configuration JSON file.  \n        model_families (list): List of model family names to retrieve.  \n  \n    Returns:  \n        dict: A dictionary of available models and their configurations for the specified families.  \n    \"\"\"  \n    configs = CONFIG.copy()\n  \n    # Extract models and their configurations for the specified families  \n    filtered_configs = {}  \n  \n    for family in model_families:  \n        if family in configs:  \n            # Check if model filtering is required  \n            if model == \"all\":  \n                # Include all models in the family  \n                for model_size, model_configs in configs[family].items():  \n                    filtered_configs[f\"{family}-{model_size}\"] = model_configs  \n            else:  \n                # Parse the model string (e.g., llama3_8B or llama3-8B)  \n                delimiter = \"_\" if \"_\" in model else \"-\"  \n                model_parts = model.split(delimiter)  \n  \n                # Check if the family and size match  \n                if len(model_parts) == 2 and model_parts[0] == family:  \n                    model_size = model_parts[1]  \n                    if model_size in configs[family]:  \n                        filtered_configs[f\"{family}-{model_size}\"] = configs[family][model_size]  \n  \n    if not filtered_configs:  \n        print(f\"Warning: No models selected for families: {model_families} with filter: '{model}'\")  \n  \n    return filtered_configs  \n  \n  \ndef get_available_models(config_file='model_configs.json', model_families=[\"llama3\"]):  \n    \"\"\"  \n    Load model names from the configuration file.  \n  \n    Args:  \n        config_file (str): Path to the configuration JSON file.  \n        model_families (list): List of model family names to retrieve.  \n  \n    Returns:  \n        list: A list of available models for the specified families.  \n    \"\"\"  \n    configs = CONFIG.copy()\n  \n    models = [f\"{family}-{model}\" for family in model_families if family in configs for model in configs[family]]  \n  \n    return models  \n\n\ndef set_seed(seed: int = 42) -> None:\n    \"\"\"\n    Set the random seed for reproducibility across multiple libraries and configure PyTorch for deterministic behavior.\n\n    Args:\n        seed (int): The seed value to set. Default is 42.\n    \"\"\"\n    # Set seed for Python's built-in random module\n    random.seed(seed)\n    # Set seed for NumPy\n    np.random.seed(seed)\n    # Set seed for PyTorch on CPU\n    torch.manual_seed(seed)\n    # Set seed for PyTorch on all GPUs (if available)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    # Ensure deterministic behavior in PyTorch\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set environment variable for hash-based operations\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n######################################## HELPERS for Eval ######################################## \n  \n\ndef softmax(x):\n    n_rows, n_cols = x.shape\n\n    MAX_FUSED_SIZE = 65536 // x.element_size()\n    BLOCK_SIZE = 256\n    y = torch.empty_like(x)\n\n    num_programs = n_rows\n\n    grid = lambda meta: (num_programs, )\n    softmax_kernel_naive[grid](\n        x,\n        y,\n        x.stride(0),\n        n_cols,\n        BLOCK_SIZE,\n    )\n\n    return y\n\n\ndef run_softmax(M, N):\n    print(f\"Running Softmax on shape ({M},{N})\")\n    set_seed()\n    x = torch.randn(M, N, device='cuda')\n    y_triton = softmax(x)\n\n    return y_triton\n\n\n#pytest\n@pytest.mark.parametrize('M, N', [(1823, 781), (1, 1), (128, 1), (1, 128), (8192, 8192), (4096, 8192), (359, 1),\n                                  (1, 359), (1, 131072), (1, 89999)])\ndef test_softmax(M, N, request):\n    set_seed()\n    x = torch.randn(M, N, device='cuda')\n    y_triton = softmax(x)\n    y_torch = torch.softmax(x, axis=1)\n    ################### save tri_out in result_gold ###################\n    test_case_name = request.node.name\n    sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\")\n    result_gold[sanitized_key_name] = y_triton.clone().detach().cpu()\n    ###################################################################\n    result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])\n    \n    assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)\n\n\n#Benchmark\narg_to_torch_dtype = {'fp16': torch.float16, 'bf16': torch.bfloat16, 'fp32': torch.float32}\n\n# --- Define TFLOPS and GB/s calculators for Softmax Forward ---\ndef calculate_softmax_fwd_gbps(params: dict, ms: float) -> float:\n    M, N = params['M'], params['N']\n    dtype_str = params.get('dtype_str', 'fp16')\n    if dtype_str == 'fp32': current_dtype = torch.float32\n    elif dtype_str == 'bf16': current_dtype = torch.bfloat16\n    else: current_dtype = torch.float16\n    element_size = torch.tensor([], dtype=current_dtype).element_size()\n    \n    # Read x (M,N), Write y (M,N)\n    # Intermediate m (M) and row_sum (M) are usually kept in registers/SRAM per row,\n    # not necessarily global memory traffic unless spilled, which is hard to model simply.\n    # For online softmax, data is read twice effectively (once for max/sum, once for normalization).\n    bytes_read_x_pass1 = M * N * element_size\n    bytes_read_x_pass2 = M * N * element_size # Or just once if fully fused and data stays in cache\n    bytes_write_y = M * N * element_size\n\n    # A common simplification for bandwidth: 2*M*N (one read, one write)\n    # More accurate for online: read_pass1 + read_pass2 + write\n    # Let's use 2 reads, 1 write for online softmax\n    total_bytes = bytes_read_x_pass1 + bytes_read_x_pass2 + bytes_write_y\n    gbps = total_bytes / (ms / 1000) / 1e9\n    return gbps\n\ndef calculate_softmax_fwd_tflops(params: dict, ms: float) -> float:\n    M, N = params['M'], params['N']\n    # FLOPs for Softmax forward (per row):\n    # 1. Find max: N-1 comparisons (approx N ops)\n    # 2. Subtract max: N subtractions (N ops)\n    # 3. Exp: N exponentials (N * ~5-10 ops, say N*5)\n    # 4. Sum exps: N-1 additions (approx N ops)\n    # 5. Divide by sum: N divisions (N ops)\n    # Total per row approx: N + N + 5N + N + N = 9N ops\n    flops_per_row = 9 * N \n    total_flops = M * flops_per_row\n    tflops = total_flops / (ms / 1000) / 1e12\n    return tflops\n\n# --- Name for the benchmark output JSON file ---\nOP_NAME_FOR_BENCHMARK = \"softmax_triton_perf\"\n\n# --- Pytest test_softmax function MODIFIED for performance benchmarking ---\n# Original parametrization is kept.\nSOFTMAX_SHAPES_FOR_PERF = [\n    (2048, 2048), (4096, 4096), (8192, 8192), # Square\n    (1, 32000), (1, 131072),                 # Typical vocab sizes (batch 1)\n    (1024, 8192), (512, 32000),              # Batch > 1\n    # (1,4), (1823, 781) # Smaller/odd shapes\n]\nSOFTMAX_DTYPES_FOR_PERF = ['fp16', 'bf16', 'fp32']\n\n\n@pytest.mark.parametrize('M, N', SOFTMAX_SHAPES_FOR_PERF)\n@pytest.mark.parametrize('dtype_str', SOFTMAX_DTYPES_FOR_PERF)\ndef test_performance(M, N, dtype_str, request): # Renamed from test_softmax\n    set_seed()\n    \n    if dtype_str == 'fp32': current_dtype = torch.float32\n    elif dtype_str == 'bf16': current_dtype = torch.bfloat16\n    else: current_dtype = torch.float16\n\n    x = torch.randn(M, N, device='cuda', dtype=current_dtype)\n\n    # --- Create op_lambda for benchmarking ---\n    op_lambda = lambda: softmax(x)\n\n    # --- Benchmarking ---\n    bench_config = do_bench_config(warm_up=25, repetition=100)\n    benchmarker = PytestBenchmarker(op_callable=op_lambda,\n                                    op_name=OP_NAME_FOR_BENCHMARK,\n                                    config=bench_config)\n\n    # Determine BLOCK_SIZE as it's done in the softmax_triton_wrapper for logging\n    # This is for logging consistency, the actual kernel uses autotuned block size.\n    # The BLOCK_SIZE passed to kernel is a key for autotuning.\n    MAX_FUSED_SIZE_log = 65536 // x.element_size()\n    BLOCK_SIZE_log = min(MAX_FUSED_SIZE_log, triton.next_power_of_2(N if N > 0 else 1))\n    if BLOCK_SIZE_log == 0: BLOCK_SIZE_log = 1\n\n\n    current_params_for_logs_and_calc = {\n        \"M\": M, \"N\": N, \"dtype_str\": dtype_str,\n        \"LOGGED_BLOCK_SIZE_heuristic\": BLOCK_SIZE_log # Log the heuristic block size\n    }\n\n    benchmarker.run_benchmark(current_params_dict=current_params_for_logs_and_calc,\n                              gbps_calculator=calculate_softmax_fwd_gbps,\n                              tflops_calculator=calculate_softmax_fwd_tflops)\n\n\ndef model_benchmark_configs(args):\n    config_file = args.model_configs\n    configs = get_model_configs(config_path=config_file, model_families=[\"llama3\"], model=args.model)\n\n    x_vals_list = []\n    batch_size = args.b if args.b else 1\n\n    for model_name, config in configs.items():\n        seq_len = args.sq if args.sq else 4096\n        x_vals_list.append((model_name, batch_size * seq_len, config[\"vocab_size\"]))\n\n    return x_vals_list\n\n\ndef run_benchmark(args):\n    config = []\n    if (args.M_benchmark):\n        val = args.M_start\n        x_vals_list = []\n        while val <= args.M_end:\n            x_vals_list.append(val)\n            val *= args.M_step\n        mn_args = {'N': args.N_start}\n        plot_name = str(\"softmax-performance_\" + args.dtype + \"_N\" + str(args.N_start) + \"_M\" + str(args.M_start) +\n                        \"-\" + str(args.M_end) + \"-\" + str(args.M_step))\n        x_names = ['M']\n    else:\n        x_vals_list = [i for i in range(args.N_start, args.N_end, args.N_step)]\n        mn_args = {'M': args.M_start}\n        plot_name = str(\"softmax-performance_\" + args.dtype + \"_M\" + str(args.M_start) + \"_N\" + str(args.N_start) +\n                        \"-\" + str(args.N_end) + \"-\" + str(args.N_step))\n        x_names = ['N']\n\n    if args.model:\n        assert not args.M_benchmark, \\\n            \"Trying to provide both -model benchmark and M_benchmark is not supported!\"\n        x_names = ['model', 'M', 'N']\n        mn_args = {}\n        plot_name = str(\"softmax-performance_\" + args.dtype)\n        x_vals_list = model_benchmark_configs(args)\n\n    dtype = arg_to_torch_dtype[args.dtype]\n\n    print(plot_name)\n    config.append(\n        triton.testing.Benchmark(\n            x_names=x_names,\n            x_vals=x_vals_list,\n            line_arg='provider',\n            line_vals=['triton', 'torch'],\n            line_names=[\n                \"Triton\",\n                \"Torch\",\n            ],\n            styles=[('blue', '-'), ('green', '-')],\n            ylabel=\"GB/s\",\n            plot_name=plot_name,\n            args=mn_args,\n        ))\n\n    @triton.testing.perf_report(config)\n    def benchmark(M, N, provider, model=None):\n        x = torch.randn(M, N, device='cuda', dtype=dtype)\n        stream = torch.cuda.Stream()\n        torch.cuda.set_stream(stream)\n        if provider == 'torch':\n            ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))\n        if provider == 'triton':\n            ms = triton.testing.do_bench(lambda: softmax(x))\n        gbps = lambda ms: 2 * x.nelement() * x.element_size() * 1e-9 / (ms * 1e-3)\n        return gbps(ms)\n\n    benchmark.run(save_path=\".\", show_plots=True, print_data=True)\n\n######################################## HELPERS for Eval ########################################     \n# --- Pytest hook to save the dictionary at the end of the session ---  \ndef test_save_results():  \n    \"\"\"  \n    Called after whole test run finished, right before returning the exit status to the system.  \n    \"\"\"\n    print('Inside session finish...')\n    if \"_CALL_SUCCESS_\" not in result_gold:\n        result_gold['_CALL_SUCCESS_'] = torch.tensor([[0.0]])\n    OUTPUT_FILENAME = __file__.replace('.','_') + '.pt'\n    print(f\"\\nSaving all y_triton results to {OUTPUT_FILENAME}...\")  \n    # Ensure the directory for the output file exists if it's in a subdirectory  \n    output_dir = os.path.dirname(OUTPUT_FILENAME)  \n    if output_dir and not os.path.exists(output_dir):  \n        os.makedirs(output_dir, exist_ok=True)  \n    torch.save(result_gold, OUTPUT_FILENAME)       \n    print(f\"Successfully saved {len(result_gold)} y_triton tensors to {OUTPUT_FILENAME}.\")  \n\ndef test_save_performance_results():\n    \"\"\"\n    Called after the test_performance function finishes.\n    This is a separate hook to ensure performance results are saved.\n    \"\"\"\n    print('\\nPytest session finishing... Saving benchmark results...')\n\n    output_directory = os.path.join(os.path.dirname(__file__), \"perf\")  # Save in a \"perf\" subdirectory next to the test file\n    os.makedirs(output_directory, exist_ok=True)\n    \n    save_all_benchmark_results(output_directory)\n    print(f\"All benchmark results attempted to save to: {output_directory}\")\n\n######################################## HELPERS for Eval ######################################## \n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        prog=\"Benchmark Softmax\",\n        allow_abbrev=False,\n    )\n    parser.add_argument('-model_configs', type=str, default=\"model_configs.json\", help=\"Model config json file.\")\n\n    available_models = get_available_models(model_families=[\"llama3\"])  # Dynamically load model names\n    model_help = (\n        \"Model name to benchmark. Select from: [\" + \", \".join(available_models) +\n        \"]. Use 'all' to benchmark all models. Not providing runs the default benchmark script with custom configs.\")\n    parser.add_argument('-model', type=str, default=None, help=model_help)\n    parser.add_argument('-b', type=int, default=0, help=\"Batch size used together with model.\")\n    parser.add_argument('-sq', type=int, default=0, help=\"Sequence length used together with model.\")\n    parser.add_argument('-M', \"--M_start\", default=\"1\", type=int)\n    parser.add_argument('-Ms', \"--M_step\", default=\"2\", type=int)\n    parser.add_argument('-Me', \"--M_end\", default=\"512\", type=int)\n    parser.add_argument('-Mb', \"--M_benchmark\", default=False, type=bool)\n\n    parser.add_argument('-N', \"--N_start\", default=\"1024\", type=int)\n    parser.add_argument('-Ns', \"--N_step\", default=\"2048\", type=int)\n    parser.add_argument('-Ne', \"--N_end\", default=\"65536\", type=int)\n\n    parser.add_argument('-d', \"--dtype\", default=\"fp16\")\n    parser.add_argument('-nb', \"--no_benchmark\", default=False, type=bool)\n\n    return parser.parse_args()\n\n\ndef main():\n    args = parse_args()\n\n    if args.no_benchmark:\n        run_softmax(args.M_start, args.N_start)\n    else:\n        run_benchmark(args)\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n"}, {"file": "test_gemm_no_scf.py", "target_kernel_name": "matmul_no_scf_kernel", "instruction": "\nYou are an expert in triton programming language. You will be given the function definition for the `matmul_no_scf_kernel` kernels. Your task is to complete the kernel code. Only complete the kernel code in the function definition, DONT remove any python imports or helper utils in the instruction/code provided, DONT change/interfere with the provided function definition and parameter list.\n\nThis kernel, `matmul_no_scf_kernel`,  performs single block of matrix multiplication (C = A @ B) without Structured Control Flow (SCF)\n**Your objective is to implement the body of both the kernels `matmul_no_scf_kernel`.**\n\nYou must ensure that:\n1.  All arguments received by `matmul_no_scf_kernel` are kept intact and not modified.\n2. Provide you final code in ```python code block. \nExample:\n```python\n<YOUR-CODE-HERE>\n```\nThe full definitions for `matmul_no_scf_kernel` and relevant helper utilities are provided in the context below. You only need to complete the code for `matmul_no_scf_kernel` whilst keeping other things intact. DONT remove Imports and HELPER utils.\n\n######################################## Imports ######################################## \nimport itertools\nimport os\nimport re\n\nimport pytest\nimport torch\nfrom torch.testing import assert_close\n\nimport triton\nimport triton.language as tl\n\n######################################## Imports ######################################## \n\n\n\n\n@triton.jit\ndef matmul_no_scf_kernel(\n    a_ptr,  # tl.pointer_type(dtype)\n    b_ptr,  # tl.pointer_type(dtype)\n    c_ptr,  # tl.pointer_type(dtype)\n    M: int,\n    N: int,\n    K: int,\n    stride_am: int,\n    stride_ak: int,\n    stride_bk: int,\n    stride_bn: int,\n    stride_cm: int,\n    stride_cn: int,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n    FLOAT16_OUTPUT: tl.constexpr,\n    USE_TMA_EPILOGUE: tl.constexpr\n):\n    \"\"\"\n    Computes a single block of matrix multiplication (C = A @ B) without explicit\n    iteration over the K dimension (i.e., no `tl.for_loop` for K accumulation,\n    implying K must be equal to BLOCK_K). This kernel is  named \"no_scf\"\n    because performing the full matmul accumulation over K would typically\n    introduce Structured Control Flow (SCF) in the generated MLIR/LLVM IR,\n    which this kernel avoids by processing only one K-block.\n\n    It loads one block of matrix A and one block of matrix B, performs a dot\n    product, and stores the resulting block to matrix C. The kernel should support\n    optional casting of the output to float16 and an optional TMA-based epilogue\n    for storing the result.\n\n    Parameters:\n    -----------\n    a_ptr : tl.pointer_type\n        Pointer to the input matrix A.\n    b_ptr : tl.pointer_type\n        Pointer to the input matrix B.\n    c_ptr : tl.pointer_type\n        Pointer to the output matrix C.\n    M : int\n        Number of rows in matrix A and C. Expected to be equal to BLOCK_M.\n    N : int\n        Number of columns in matrix B and C. Expected to be equal to BLOCK_N.\n    K : int\n        Number of columns in matrix A and rows in matrix B (the common dimension).\n        Expected to be equal to BLOCK_K.\n    stride_am : int\n        Stride of matrix A along the M dimension (row stride).\n    stride_ak : int\n        Stride of matrix A along the K dimension (column stride).\n    stride_bk : int\n        Stride of matrix B along the K dimension (row stride).\n    stride_bn : int\n        Stride of matrix B along the N dimension (column stride).\n    stride_cm : int\n        Stride of matrix C along the M dimension (row stride).\n    stride_cn : int\n        Stride of matrix C along the N dimension (column stride).\n    BLOCK_M : tl.constexpr\n        Compile-time constant defining the height of the blocks to be processed from\n        matrices A and C.\n    BLOCK_N : tl.constexpr\n        Compile-time constant defining the width of the blocks to be processed from\n        matrices B and C.\n    BLOCK_K : tl.constexpr\n        Compile-time constant defining the width of the block from matrix A and\n        the height of the block from matrix B (common dimension for dot product).\n    FLOAT16_OUTPUT : tl.constexpr\n        Compile-time boolean constant. If True, the output matrix C will be cast\n        to float16 before storing. Otherwise, it will be stored in the compute\n        precision (e.g., float32).\n    USE_TMA_EPILOGUE : tl.constexpr\n        Compile-time boolean constant. If True, the epilogue (storing the result C)\n        will use Tensor Memory Access (TMA) operations via `tl.make_block_ptr`\n        and `tl.store`. If False, it will use a more traditional epilogue by\n        calculating destination pointers manually with `tl.arange` and `tl.store`.\n    \"\"\"\n    # Your code here\n\n", "label": "# Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n#\n# Permission is hereby granted, free of charge, to any person obtaining\n# a copy of this software and associated documentation files\n# (the \"Software\"), to deal in the Software without restriction,\n# including without limitation the rights to use, copy, modify, merge,\n# publish, distribute, sublicense, and/or sell copies of the Software,\n# and to permit persons to whom the Software is furnished to do so,\n# subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be\n# included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n# MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n# IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n# CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n# SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n######################################## Imports ######################################## \nimport itertools\nimport os\nimport re\n\nimport pytest\nimport torch\nfrom torch.testing import assert_close\n\nimport triton\nimport triton.language as tl\n\n######################################## Imports ######################################## \n\n\n\n@triton.jit\ndef matmul_no_scf_kernel(a_ptr, b_ptr, c_ptr,  #\n                         M, N, K,  #\n                         stride_am, stride_ak,  #\n                         stride_bk, stride_bn,  #\n                         stride_cm, stride_cn,  #\n                         BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,  #\n                         FLOAT16_OUTPUT: tl.constexpr, USE_TMA_EPILOGUE: tl.constexpr  #\n                         ):\n    a_block_ptr = tl.make_block_ptr(\n        base=a_ptr,\n        shape=(M, K),\n        strides=(stride_am, stride_ak),\n        offsets=(0, 0),\n        block_shape=(BLOCK_M, BLOCK_K),\n        order=(1, 0),\n    )\n    b_block_ptr = tl.make_block_ptr(\n        base=b_ptr,\n        shape=(K, N),\n        strides=(stride_bk, stride_bn),\n        offsets=(0, 0),\n        block_shape=(BLOCK_K, BLOCK_N),\n        order=(0, 1),\n    )\n    a = tl.load(a_block_ptr)\n    b = tl.load(b_block_ptr)\n\n    c = tl.dot(a, b)\n\n    if FLOAT16_OUTPUT:\n        c = c.to(tl.float16)\n\n    if USE_TMA_EPILOGUE:\n        c_block_ptr = tl.make_block_ptr(base=c_ptr, shape=(M, N), strides=(stride_cm, stride_cn), offsets=(0, 0),\n                                        block_shape=(BLOCK_M, BLOCK_N), order=(1, 0))\n        tl.store(c_block_ptr, c)\n    else:\n        offs_m = tl.arange(0, BLOCK_M)\n        offs_n = tl.arange(0, BLOCK_N)\n        c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n        tl.store(c_ptrs, c)\n\n##################################################################################################################################################  \n\nimport numpy as np\nimport random\nimport torch \nimport os\nimport pytest\nfrom numpy.random import RandomState\nimport itertools\nimport re\n\nfrom torch.testing import assert_close\nfrom tb_eval.perf.ROCm.performance_utils_pytest import PytestBenchmarker, do_bench_config, save_all_benchmark_results\nfrom typing import Dict\nimport triton\nimport triton.language as tl\n\n\nresult_gold = {}\n\n######################################## HELPERS for Eval ######################################## \ndef set_seed(seed: int = 42) -> None:\n    \"\"\"\n    Set the random seed for reproducibility across multiple libraries and configure PyTorch for deterministic behavior.\n\n    Args:\n        seed (int): The seed value to set. Default is 42.\n    \"\"\"\n    # Set seed for Python's built-in random module\n    random.seed(seed)\n    # Set seed for NumPy\n    np.random.seed(seed)\n    # Set seed for PyTorch on CPU\n    torch.manual_seed(seed)\n    # Set seed for PyTorch on all GPUs (if available)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    # Ensure deterministic behavior in PyTorch\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set environment variable for hash-based operations\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\ndef is_hip():\n    return triton.runtime.driver.active.get_current_target().backend == \"hip\"\n\n\n######################################## HELPERS for Eval ######################################## \n\n\n\n\n@pytest.mark.parametrize(\n    'M,N,K,NUM_CTAS,NUM_WARPS,TRANS_A,TRANS_B,OUTPUT_TYPE,USE_TMA_EPILOGUE',\n    itertools.chain(*[[\n        # numCTAs = 1, no TMA multicast:\n        [64, 16, 16, 1, 4, False, True, \"float16\", USE_TMA_EPILOGUE],\n        [64, 32, 16, 1, 4, False, True, \"float16\", USE_TMA_EPILOGUE],\n        [64, 64, 16, 1, 4, False, True, \"float16\", USE_TMA_EPILOGUE],\n        [64, 64, 16, 1, 4, False, True, \"float32\", USE_TMA_EPILOGUE],\n        [64, 64, 32, 1, 4, False, True, \"float32\", USE_TMA_EPILOGUE],\n        [64, 64, 64, 1, 4, False, True, \"float32\", USE_TMA_EPILOGUE],\n        [128, 128, 16, 1, 4, False, True, \"float16\", USE_TMA_EPILOGUE],\n        [128, 128, 16, 1, 4, False, True, \"float32\", USE_TMA_EPILOGUE],\n        # static mask, cluster 4x1\n        [256, 64, 16, 4, 4, False, True, \"float16\", USE_TMA_EPILOGUE],\n        [256, 64, 16, 4, 4, False, True, \"float32\", USE_TMA_EPILOGUE],\n        # dynamic mask, cluster 2x2\n        [128, 128, 16, 4, 4, False, True, \"float16\", USE_TMA_EPILOGUE],\n        [128, 128, 16, 4, 4, False, True, \"float32\", USE_TMA_EPILOGUE],\n        # small M, N\n        [16, 16, 16, 1, 4, False, True, \"float32\", USE_TMA_EPILOGUE],\n        [16, 32, 16, 1, 4, False, True, \"float32\", USE_TMA_EPILOGUE],\n        [32, 16, 16, 1, 4, False, True, \"float32\", USE_TMA_EPILOGUE],\n        [32, 32, 16, 1, 4, False, True, \"float32\", USE_TMA_EPILOGUE],\n    ] for USE_TMA_EPILOGUE in [True, False]]))\n@pytest.mark.skipif(torch.cuda.get_device_capability()[0] < 9, reason=\"Requires compute capability >= 9\")\ndef test_gemm_no_scf(M, N, K, NUM_CTAS, NUM_WARPS, TRANS_A, TRANS_B, OUTPUT_TYPE, USE_TMA_EPILOGUE, request):\n    set_seed()\n    \n    if is_hip() and NUM_CTAS > 1:\n        pytest.skip(\"NUM_CTAS > 1 is not supported in HIP backend\")\n\n    if (TRANS_A):\n        a = torch.randn((K, M), device='cuda', dtype=torch.float16).T\n    else:\n        a = torch.randn((M, K), device='cuda', dtype=torch.float16)\n    if (TRANS_B):\n        b = torch.randn((N, K), device='cuda', dtype=torch.float16).T\n    else:\n        b = torch.randn((K, N), device='cuda', dtype=torch.float16)\n\n    if OUTPUT_TYPE == \"float16\":\n        c = torch.empty((M, N), device=a.device, dtype=torch.float16)\n    else:\n        c = torch.empty((M, N), device=a.device, dtype=torch.float32)\n\n    matmul_no_scf_kernel[(1, 1)](\n        a_ptr=a, b_ptr=b, c_ptr=c,  #\n        M=M, N=N, K=K,  #\n        stride_am=a.stride(0), stride_ak=a.stride(1),  #\n        stride_bk=b.stride(0), stride_bn=b.stride(1),  #\n        stride_cm=c.stride(0), stride_cn=c.stride(1),  #\n        BLOCK_M=M, BLOCK_N=N, BLOCK_K=K,  #\n        num_warps=NUM_WARPS,  #\n        num_ctas=NUM_CTAS,  #\n        FLOAT16_OUTPUT=(OUTPUT_TYPE == \"float16\"),  #\n        USE_TMA_EPILOGUE=USE_TMA_EPILOGUE)\n    a_f32 = a.to(torch.float32)\n    b_f32 = b.to(torch.float32)\n    golden = torch.matmul(a_f32, b_f32)\n    torch.set_printoptions(profile=\"full\")\n\n    result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])\n    \n    ################### save tri_out in result_gold ###################\n    test_case_name = request.node.name\n    sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\")\n    result_gold[sanitized_key_name] = c.clone().detach().cpu()\n    ###################################################################\n\n\n    assert_close(c, golden, rtol=1e-2, atol=1e-3, check_dtype=False)\n\n\ndef gemm_no_scf_triton_wrapper(a_tensor, b_tensor, c_buffer, \n                               M_dim, N_dim, K_dim, \n                               num_warps_launch, float16_out_flag, use_tma_flag):\n    grid = (1,) \n    matmul_no_scf_kernel[grid](\n        a_ptr=a_tensor, b_ptr=b_tensor, c_ptr=c_buffer,\n        M=M_dim, N=N_dim, K=K_dim,\n        stride_am=a_tensor.stride(0), stride_ak=a_tensor.stride(1),\n        stride_bk=b_tensor.stride(0), stride_bn=b_tensor.stride(1),\n        stride_cm=c_buffer.stride(0), stride_cn=c_buffer.stride(1),\n        BLOCK_M=M_dim, BLOCK_N=N_dim, BLOCK_K=K_dim, \n        FLOAT16_OUTPUT=float16_out_flag,\n        USE_TMA_EPILOGUE=use_tma_flag,\n        num_warps=num_warps_launch\n    )\n    return c_buffer\n\ndef calculate_gemm_no_scf_tflops(params: dict, ms: float) -> float:\n    M, N, K = params['M'], params['N'], params['K']\n    flops = 2 * M * N * K \n    tflops = flops / (ms / 1000) / 1e12\n    return tflops\n\n\ndef calculate_gemm_no_scf_gbps(params: dict, ms: float) -> float:\n    M, N, K = params['M'], params['N'], params['K']\n    \n    input_dtype_str = params.get('input_dtype_str', 'fp16') \n    output_dtype_str = params['OUTPUT_TYPE'] \n\n    # Convert dtype strings to torch.dtype objects\n    current_input_dtype = torch.float16 \n    if input_dtype_str == 'fp32': current_input_dtype = torch.float32\n    elif input_dtype_str == 'bf16': current_input_dtype = torch.bfloat16\n\n    current_output_dtype = torch.float16 \n    if output_dtype_str == 'fp32': current_output_dtype = torch.float32\n    elif output_dtype_str == 'bf16': current_output_dtype = torch.bfloat16\n    \n    # Get element sizes by creating dummy tensors\n    in_element_size = torch.tensor([], dtype=current_input_dtype).element_size()\n    out_element_size = torch.tensor([], dtype=current_output_dtype).element_size()\n    \n    bytes_a = M * K * in_element_size\n    bytes_b = K * N * in_element_size \n    bytes_c_write = M * N * out_element_size\n    total_bytes = bytes_a + bytes_b + bytes_c_write\n    gbps = total_bytes / (ms / 1000) / 1e9\n    return gbps\n\nOP_NAME_FOR_BENCHMARK = \"gemm_no_scf_triton_perf\"\n\n# --- NEW Performance Test Function using original test_gemm_no_scf's parametrization ---\n@pytest.mark.parametrize(\n    'M,N,K,NUM_CTAS,NUM_WARPS,TRANS_A,TRANS_B,OUTPUT_TYPE,USE_TMA_EPILOGUE',\n    # Using a smaller, more targeted subset for performance to avoid excessive shared memory issues\n    # and long runtimes. The original parametrize is very extensive.\n    # Focus on K values that are likely to fit for a single-block kernel.\n    itertools.chain(*[[\n        # K=16\n        [64, 64, 16, 1, 4, False, False, \"float16\", USE_TMA_EPILOGUE], # A(M,K), B(K,N)\n        [64, 64, 16, 1, 4, False, False, \"float32\", USE_TMA_EPILOGUE],\n        [128, 128, 16, 1, 4, False, False, \"float16\", USE_TMA_EPILOGUE],\n        # K=32\n        [64, 64, 32, 1, 4, False, False, \"float16\", USE_TMA_EPILOGUE],\n        [64, 64, 32, 1, 4, False, False, \"float32\", USE_TMA_EPILOGUE],\n        [128, 64, 32, 1, 4, False, False, \"float16\", USE_TMA_EPILOGUE], # M=128, N=64, K=32\n        # K=64\n        [32, 32, 64, 1, 4, False, False, \"float16\", USE_TMA_EPILOGUE], # M=32, N=32, K=64\n        [64, 64, 64, 1, 4, False, False, \"float16\", USE_TMA_EPILOGUE],\n        [64, 64, 64, 1, 4, False, False, \"float32\", USE_TMA_EPILOGUE],\n        # K=128 - starts to get large for single block shared mem\n        [32, 32, 128, 1, 4, False, False, \"float16\", USE_TMA_EPILOGUE],\n        # [64, 64, 128, 1, 4, False, False, \"float16\", USE_TMA_EPILOGUE], # (32*128+128*32)*2 = 16384. Ok.\n                                                                        # (64*128+128*64)*2 = 32768. Ok.\n    ] for USE_TMA_EPILOGUE in [True, False]]))\n# @pytest.mark.skipif(torch.cuda.get_device_capability()[0] < 9, reason=\"Requires compute capability >= 9\")\ndef test_performance(M, N, K, NUM_CTAS, NUM_WARPS, TRANS_A, TRANS_B, OUTPUT_TYPE, USE_TMA_EPILOGUE, request):\n    cap = torch.cuda.get_device_capability()\n    if cap[0] < 9:\n        pytest.skip(\"Requires compute capability >= 9 (as per original test)\")\n    if is_hip() and NUM_CTAS > 1: \n        pytest.skip(\"NUM_CTAS > 1 is not supported in HIP for this test (as per original)\")\n\n    # Shared memory check (for fp16 inputs)\n    input_element_size = 2 # Assuming fp16 inputs\n    smem_elements_needed = (M * K) + (K * N)\n    if smem_elements_needed * input_element_size > 65536: # 64KB limit\n        pytest.skip(f\"Skipping M{M}N{N}K{K} due to estimated shared memory for inputs: \"\n                    f\"{smem_elements_needed * input_element_size} > 65536\")\n\n    set_seed()\n    \n    input_torch_dtype = torch.float16 # Original test uses fp16 for a and b\n\n    # Input setup: Ensure a is (M,K) and b is (K,N) before passing to wrapper\n    a_shape_before_trans = (K, M) if TRANS_A else (M, K)\n    b_shape_before_trans = (N, K) if TRANS_B else (K, N)\n    \n    a_host = torch.randn(a_shape_before_trans, device='cuda', dtype=input_torch_dtype)\n    if TRANS_A: a_host = a_host.T \n    \n    b_host = torch.randn(b_shape_before_trans, device='cuda', dtype=input_torch_dtype)\n    if TRANS_B: b_host = b_host.T \n\n    c_out_torch_dtype = torch.float16 if OUTPUT_TYPE == \"float16\" else torch.float32\n    c_buffer = torch.empty((M, N), device=a_host.device, dtype=c_out_torch_dtype)\n\n    op_lambda = lambda: gemm_no_scf_triton_wrapper(\n        a_host, b_host, c_buffer, M, N, K, \n        NUM_WARPS, (OUTPUT_TYPE == \"float16\"), USE_TMA_EPILOGUE\n    )\n\n    bench_config = do_bench_config(warm_up=25, repetition=100)\n    benchmarker = PytestBenchmarker(op_callable=op_lambda,\n                                    op_name=OP_NAME_FOR_BENCHMARK,\n                                    config=bench_config)\n    current_params_for_logs_and_calc = {\n        \"M\": M, \"N\": N, \"K\": K, \"NUM_CTAS\": NUM_CTAS, \"NUM_WARPS\": NUM_WARPS,\n        \"TRANS_A\": TRANS_A, \"TRANS_B\": TRANS_B, \n        \"OUTPUT_TYPE\": OUTPUT_TYPE, \"USE_TMA_EPILOGUE\": USE_TMA_EPILOGUE,\n        \"input_dtype_str\": \"fp16\" # Hardcoded based on original test's a,b creation\n    }\n    \n    perf_result = benchmarker.run_benchmark(current_params_dict=current_params_for_logs_and_calc,\n                                            gbps_calculator=calculate_gemm_no_scf_gbps,\n                                            tflops_calculator=calculate_gemm_no_scf_tflops)\n\n\n######################################## HELPERS for Eval ########################################     \n# --- Pytest hook to save the dictionary at the end of the session ---  \ndef test_save_results():  \n    \"\"\"  \n    Called after whole test run finished, right before returning the exit status to the system.  \n    \"\"\"\n    print('Inside session finish...')\n    if \"_CALL_SUCCESS_\" not in result_gold:\n        result_gold['_CALL_SUCCESS_'] = torch.tensor([[0.0]])\n    OUTPUT_FILENAME = __file__.replace('.','_') + '.pt'\n    print(f\"\\nSaving all y_triton results to {OUTPUT_FILENAME}...\")  \n    # Ensure the directory for the output file exists if it's in a subdirectory  \n    output_dir = os.path.dirname(OUTPUT_FILENAME)  \n    if output_dir and not os.path.exists(output_dir):  \n        os.makedirs(output_dir, exist_ok=True)  \n    torch.save(result_gold, OUTPUT_FILENAME)       \n    print(f\"Successfully saved {len(result_gold)} y_triton tensors to {OUTPUT_FILENAME}.\")  \n\n\ndef test_save_performance_results():\n    \"\"\"\n    Called after the test_performance function finishes.\n    This is a separate hook to ensure performance results are saved.\n    \"\"\"\n    print('\\nPytest session finishing... Saving benchmark results...')\n\n    output_directory = os.path.join(os.path.dirname(__file__), \"perf\")  # Save in a \"perf\" subdirectory next to the test file\n    os.makedirs(output_directory, exist_ok=True)\n    \n    save_all_benchmark_results(output_directory)\n    print(f\"All benchmark results attempted to save to: {output_directory}\")\n######################################## HELPERS for Eval ########################################"}]